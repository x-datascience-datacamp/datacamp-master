{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting case-study: Bike riding in Paris\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/x-datascience-datacamp/datacamp-master/blob/main/06_feature_engineering/04-forecasting-bikes.ipynb)\n",
    "\n",
    "```\n",
    "Authors: Thomas Moreau\n",
    "         Pedro Rodrigues\n",
    "         Mathurin Massias\n",
    "         Alexandre Gramfort\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "In the recent years, bikes have been used in much larger proportion to move world-wide, and in particular in Paris. They account for 15% of the every day travel inside the city and the number of bike travel increased by 70% in the last year. Today, there exists great interest in studying the bike flux to their important role in traffic, environmental and health issues. In particular, it is an important information for the city council to know the number of bikes that will travel on different axis in the next days, weeks or months to plan the city's infrastructure.\n",
    "\n",
    "### Attribute Information\n",
    "\n",
    "The dataset was collected with cyclist counters installed by Paris city council in multiple locations. It contains hourly information about cyclist traffic, as well as the following features:\n",
    "\n",
    "- counter name\n",
    "- counter site name\n",
    "- date\n",
    "- counter installation date\n",
    "- latitude and longitude\n",
    "\n",
    "Available features are quite scarce. However, we can also use any **external data** that can help you to predict the target variable.\n",
    "\n",
    "### Objective: forecast the number of bike rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the parquet format. We do not use a .csv file as parquet files are a lot faster to load and it produces much smaller files. It also allows to store the proper dtype of each column. You should consider it for your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/bike-counter-data.parquet'\n",
    "URL_REPO = \"https://github.com/x-datascience-datacamp/datacamp-master/raw/main/06_feature_engineering/\"\n",
    "\n",
    "if Path(DATA_FILE).exists():\n",
    "    df = pd.read_parquet(DATA_FILE)\n",
    "else:\n",
    "    df = pd.read_parquet(f\"{URL_REPO}{DATA_FILE}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first look at the target one wants to predict\n",
    "\n",
    "One can see that the target is quite heavy tailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.histplot(df, x='bike_count', kde=True, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the most used sites. Note that one can have 2 counters in one site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['site_name', 'counter_name'], observed=True)['bike_count'].sum().sort_values(\n",
    "    ascending=False\n",
    ").head(10).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "m = folium.Map(location=df[['latitude', 'longitude']].mean(axis=0).values, zoom_start=13)\n",
    "\n",
    "for _, row in df[['counter_name', 'latitude', 'longitude']].drop_duplicates('counter_name').iterrows():\n",
    "    folium.Marker(\n",
    "    row[['latitude', 'longitude']].values.tolist(), popup=row['counter_name']\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look into the temporal distribution of the most frequently used bike counters. If we plot it directly we will not see much because there are half a million data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"counter_name == 'Totem 73 boulevard de Sébastopol S-N'\"\n",
    "_ = df.query(query_str).sort_values('date').plot(x='date', y='bike_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we aggregate the data, for instance, by week to have a clearer overall picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.query(query_str).groupby(pd.Grouper(freq='1W', key='date'))['bike_count'].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While at the same time, we can zoom on a week in particular for a more short-term visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "df.query(query_str + \" and '2021/03/01' < date < '2021/03/08'\").sort_values('date').plot(x='date', y='bike_count', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>Plot the average bike_count per hour of the week (averaged over all sites).</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "You should clearly distinguish the commute patterns in the morning and evenings of the work days\n",
    "and the leisure use of the bikes on the weekends with a more spread peak demand around the middle of the days.\n",
    "\n",
    "Solution is in `solutions/04-1-case_study_averaging_per_weekday_hour.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "There are many possible metrics for regression tasks like here. In what follows we will be using the RMSLE, that penalizes less large deviations than the MLE.\n",
    "\n",
    "#### Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "RMSLE = $\\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (\\log(x_i + 1) - \\log(y_i + 1))^2 }$\n",
    "\n",
    "In what follows we will be using the RMSLE, that penalizes less large deviations than the MSE. For this we will be creating a new column called `log_bike_count` using the `np.log1p` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_bike_count'] = np.log1p(df['bike_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X.loc[:, 'year'] = X['date'].dt.year\n",
    "    X.loc[:, 'month'] = X['date'].dt.month\n",
    "    X.loc[:, 'day'] = X['date'].dt.day\n",
    "    X.loc[:, 'weekday'] = X['date'].dt.weekday\n",
    "    X.loc[:, 'hour'] = X['date'].dt.hour\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_encode_dates(df[['date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "date_encoder = FunctionTransformer(_encode_dates, validate=False)\n",
    "date_encoder.fit_transform(df[['date']]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test data\n",
    "\n",
    "We will now focus on one site and focus on the forecast of its bike count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_name = 'Totem 73 boulevard de Sébastopol S-N'\n",
    "\n",
    "data = df.query(\"counter_name == @counter_name\").sort_values('date')\n",
    "data.plot(x='date', y=\"bike_count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop(['bike_count', 'log_bike_count'], axis='columns'), data['log_bike_count']\n",
    "X = X[['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_train = len(X) // 2\n",
    "X_train, y_train = X[:n_samples_train], y.iloc[:n_samples_train]\n",
    "X_test, y_test = X[n_samples_train:], y.iloc[n_samples_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "date_encoder = FunctionTransformer(_encode_dates)\n",
    "regressor = Ridge()\n",
    "selector = FunctionTransformer(\n",
    "    lambda X: X[['weekday', 'hour']]\n",
    ")\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "pipe = make_pipeline(date_encoder, selector, ohe, regressor)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "print(f'Train set, RMSE={root_mean_squared_error(y_train, pipe.predict(X_train)):.2f}')\n",
    "print(f'Test set, RMSE={root_mean_squared_error(y_test, pipe.predict(X_test)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((X_test['date'] > pd.to_datetime('2021/03/29'))\n",
    "        & (X_test['date'] < pd.to_datetime('2021/04/05')))\n",
    "\n",
    "df_viz = X_test.loc[mask].copy()\n",
    "df_viz['bike_count'] = np.exp(y_test[mask.values]) - 1\n",
    "df_viz['bike_count (predicted)'] = np.exp(pipe.predict(X_test[mask])) -  1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "df_viz.plot(x='date', y='bike_count', ax=ax)\n",
    "df_viz.plot(x='date', y='bike_count (predicted)', ax=ax, ls='--')\n",
    "ax.set_title('Predictions with Ridge')\n",
    "ax.set_ylabel('bike_count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the daily trend, and some of the week day differences are accounted for, however we still miss the details and the spikes in the morning and the evening are under-estimated.\n",
    "\n",
    "A useful way to visualize the error is to plot y_pred as a function of y_true,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_viz = pd.DataFrame({'y_true': y_test, 'y_pred': pipe.predict(X_test)})\n",
    "df_viz.plot.scatter(x=\"y_true\", y=\"y_pred\", s=8, alpha=0.25, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a proper cross-validation with `TimeSeriesSplit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "# When using a scorer in scikit-learn it always needs to be better when smaller, hence the minus sign.\n",
    "scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='neg_root_mean_squared_error')\n",
    "print('RMSE: ', -scores)\n",
    "print(f'RMSE (all folds): {-scores.mean():.3} ± {(-scores).std():.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>Modify the _encode_dates function to take into account the French holidays. And reevaluate the full pipeline above. Does it help significantly?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/04-2-french_holidays.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, EasterMonday, Easter\n",
    "from pandas.tseries.offsets import Day, CustomBusinessDay\n",
    "\n",
    "class FrBusinessCalendar(AbstractHolidayCalendar):\n",
    "    \"\"\" Custom Holiday calendar for France based on\n",
    "        https://en.wikipedia.org/wiki/Public_holidays_in_France\n",
    "      - 1 January: New Year's Day\n",
    "      - Moveable: Easter Monday (Monday after Easter Sunday)\n",
    "      - 1 May: Labour Day\n",
    "      - 8 May: Victory in Europe Day\n",
    "      - Moveable Ascension Day (Thursday, 39 days after Easter Sunday)\n",
    "      - 14 July: Bastille Day\n",
    "      - 15 August: Assumption of Mary to Heaven\n",
    "      - 1 November: All Saints' Day\n",
    "      - 11 November: Armistice Day\n",
    "      - 25 December: Christmas Day\n",
    "    \"\"\"\n",
    "    rules = [\n",
    "        Holiday('New Years Day', month=1, day=1),\n",
    "        EasterMonday,\n",
    "        Holiday('Labour Day', month=5, day=1),\n",
    "        Holiday('Victory in Europe Day', month=5, day=8),\n",
    "        Holiday('Ascension Day', month=1, day=1, offset=[Easter(), Day(39)]),\n",
    "        Holiday('Bastille Day', month=7, day=14),\n",
    "        Holiday('Assumption of Mary to Heaven', month=8, day=15),\n",
    "        Holiday('All Saints Day', month=11, day=1),\n",
    "        Holiday('Armistice Day', month=11, day=11),\n",
    "        Holiday('Christmas Day', month=12, day=25)\n",
    "    ]\n",
    "\n",
    "cal = FrBusinessCalendar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>Add interaction terms between one-hot encoded hour and weekdays in your model using <code>sklearn.preprocessing.PolynomialFeatures</code>. Does it help?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/04-3-polynomial_features.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch-based approaches: the AR model\n",
    "\n",
    "So far we have only explored non-recurrent forecasting models.\n",
    "Another approach is to consider a recurrent model, which learns $p(x_{t+1}| x_t, ... x_{t-p})$ and use this model by reinjecting the predicted value to predict $x_{t+2}$.\n",
    "\n",
    "Let us illustrate this concept on a simple example, where the time-series is $x_t = t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import hankel\n",
    "\n",
    "p = 3\n",
    "x = np.arange(1, 25)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can for a model with all windows of length $p=3$ to learn the next value.\n",
    "This amounts to extracting all windows of length 3 in the signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hankel(x, r=np.zeros(p))[:-p]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our features $X$, now we need the target variable $y$:\n",
    "\n",
    "*Note:* We work so that the number of samples is kept constant.\n",
    "It's a hard constraint of scikit-learn that any transformation\n",
    "to data keeps the number of samples fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[p:]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, x, n_samples, order=None):\n",
    "    if order is None:\n",
    "        order = model.coef_.shape[0]\n",
    "    y_pred = np.empty(n_samples + order)  # the prediction\n",
    "    y_pred[:order] = x[-order:]\n",
    "    for i in range(n_samples):  # now predict successively one sample at a time\n",
    "        y_pred[i + order] = model.predict(y_pred[i:i + order][None, :])[0]\n",
    "    return y_pred[order:]\n",
    "\n",
    "x_forecast = forecast(model, x, 10)\n",
    "x_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "t = np.arange(T) / 3\n",
    "x = np.sin(t)\n",
    "X, y = hankel(x, r=np.zeros(p))[:-p], x[p:]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "H = 25\n",
    "t_next = np.arange(T, T+H) / 3\n",
    "x_forecast = forecast(model, x, H)\n",
    "\n",
    "plt.plot(t, x)\n",
    "plt.plot(t_next, x_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "    Compare the performance of an auto-regressive (AR) Ridge model with an AR model based on RandomForest\n",
    "     <ol>\n",
    "      <li>Form the training dataset <code>X_ar, y_ar</code> as hankel matrices from the signal we want to predict and the next time point.</li>\n",
    "      <li>Train a <code>Ridge</code> model and a <code>RandomForestRegressor</code> with these training set.</li>\n",
    "      <li>Predict the bike demand using the <code>forecast</code> function.</li>\n",
    "      <li>Evalute the forecasted series with the <code>root_mean_square_error</code> and plot the forecast for the week of the 5th to the 12th April 2021.</li>\n",
    "    </ol>\n",
    "    \n",
    "<i>Hint:</i> Use an AR model of order $p=167$ for a week.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Solution are in `solutions/04-4-forecast-ridge.py` and `solutions/04-4-forecast-rf.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a cross validation with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for idx_train, idx_test in cv.split(X_train):\n",
    "    X_ar, y_ar = hankel(y_train.iloc[idx_train].values, r=np.zeros(p))[:-p], y_train.iloc[idx_train].values[p:]\n",
    "    model = Ridge().fit(X_ar, y_ar)\n",
    "    y_forecast = forecast(model, y_ar, len(idx_test), p)\n",
    "    scores.append(root_mean_squared_error(y_train.iloc[idx_test].values, y_forecast))\n",
    "\n",
    "scores = np.array(scores)\n",
    "print('RMSE: ', scores)\n",
    "print(f'RMSE (all folds): {scores.mean():.3} ± {(scores).std():.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using recent advances with Time-series Foundation Models\n",
    "\n",
    "As discussed with Tabular foundation models, recent trends is to replace learning a model with foundation models doing in context learning.\n",
    "This trend has also started on Time-series forecasting with various Time-Series Foundation models (TSFM) which have been proposed in 2025.\n",
    "Here we describe how to use 2 of them on our problem.\n",
    "\n",
    "## Example 1 — TiRex (NX-AI)\n",
    "\n",
    "TiRex is a compact, zero-shot forecasting foundation model based on xLSTM.\n",
    "It accepts a history window and produces point and quantile forecasts\n",
    "without training on the new dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tirex-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tirex import load_model\n",
    "\n",
    "# 1. Load the pre-trained TiRex model\n",
    "# We use the base model \"NX-AI/TiRex\".\n",
    "# If you don't have a GPU, it will fallback to CPU (may be slower for long contexts).\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tirex_model = load_model(\"NX-AI/TiRex\", device=device)\n",
    "\n",
    "# 2. Prepare Data\n",
    "# TiRex expects a torch tensor of shape (batch_size, sequence_length)\n",
    "# We convert our context data (raw values) to a float32 tensor and add a batch dimension\n",
    "context_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(0)\n",
    "context_tensor = context_tensor.to(device)\n",
    "\n",
    "# 3. Generate Forecast\n",
    "# TiRex returns a tuple: (quantiles, mean_prediction)\n",
    "# quantiles shape: (batch, prediction_length, num_quantiles)\n",
    "# mean shape: (batch, prediction_length)\n",
    "with torch.no_grad():\n",
    "    _, forecast_tirex = tirex_model.forecast(\n",
    "        context=context_tensor,\n",
    "        prediction_length=len(y_test)\n",
    "    )\n",
    "\n",
    "# Extract the mean forecast for the first (and only) batch item and move to CPU\n",
    "tirex_pred = forecast_tirex[0].cpu().numpy()\n",
    "\n",
    "# Plotting\n",
    "df_viz = X_test.loc[mask].copy()\n",
    "df_viz['bike_count'] = np.exp(y_test[mask.values]) - 1\n",
    "df_viz['bike_count (predicted)'] = np.exp(tirex_pred[mask]) -  1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "df_viz.plot(x='date', y='bike_count', ax=ax)\n",
    "df_viz.plot(x='date', y='bike_count (predicted)', ax=ax, ls='--')\n",
    "ax.set_title('Predictions with Ridge')\n",
    "ax.set_ylabel('bike_count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for idx_train, idx_test in cv.split(X_train):\n",
    "    print(f\" Scoring fold {len(scores) + 1}\")\n",
    "    context = torch.tensor(y_train.iloc[idx_train].values, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        _, forecast_tirex = tirex_model.forecast(\n",
    "            context=context_tensor,\n",
    "            prediction_length=len(idx_test)\n",
    "        )\n",
    "    y_forecast = forecast_tirex[0].cpu().numpy()\n",
    "    scores.append(root_mean_squared_error(y_train.iloc[idx_test].values, y_forecast))\n",
    "\n",
    "scores = np.array(scores)\n",
    "print('RMSE: ', scores)\n",
    "print(f'RMSE (all folds): {scores.mean():.3} ± {(scores).std():.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 -- Chronos (Amazon)\n",
    "\n",
    "Chronos (based on T5) treats time-series values as a language of tokens. It is highly robust for univariate forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chronos-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# 1. Load the pre-trained Chronos model (using 'small' variant)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "chronos_pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# 2. Prepare Data\n",
    "# Chronos expects a tensor of shape (num_series, sequence_length)\n",
    "context_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# 3. Generate Forecast\n",
    "# pipeline.predict returns samples of shape (num_series, num_samples, prediction_length)\n",
    "forecast_samples = chronos_pipeline.predict(context_tensor, len(y_test))\n",
    "\n",
    "# Extract the median forecast as the point prediction\n",
    "chronos_pred = torch.quantile(forecast_samples, 0.5, dim=1).squeeze(0).numpy()\n",
    "\n",
    "# Plotting (Matching your visualization style)\n",
    "df_viz_chronos = X_test.loc[mask].copy()\n",
    "df_viz_chronos['bike_count'] = np.exp(y_test[mask.values]) - 1\n",
    "df_viz_chronos['bike_count (predicted)'] = np.exp(chronos_pred[mask]) - 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "df_viz_chronos.plot(x='date', y='bike_count', ax=ax, label='Actual')\n",
    "df_viz_chronos.plot(x='date', y='bike_count (predicted)', ax=ax, ls='--', label='Chronos (Zero-Shot)')\n",
    "ax.set_title('Amazon Chronos: Zero-Shot Forecast')\n",
    "ax.set_ylabel('bike_count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_chronos = []\n",
    "for idx_train, idx_test in cv.split(X_train):\n",
    "    print(f\" Scoring fold {len(scores_chronos) + 1} with Chronos...\")\n",
    "    context = torch.tensor(y_train.iloc[idx_train].values, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        forecast = chronos_pipeline.predict(context, len(idx_test))\n",
    "    \n",
    "    y_forecast = torch.quantile(forecast, 0.5, dim=1).squeeze(0).numpy()\n",
    "    scores_chronos.append(root_mean_squared_error(y_train.iloc[idx_test].values, y_forecast))\n",
    "\n",
    "scores_chronos = np.array(scores_chronos)\n",
    "print('Chronos RMSE: ', scores_chronos)\n",
    "print(f'RMSE (all folds): {scores_chronos.mean():.3} ± {(scores_chronos).std():.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
