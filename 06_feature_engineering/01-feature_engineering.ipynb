{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Author : [Alexandre Gramfort](http://alexandre.gramfort.net), [Thomas Moreau](https://tommoral.github.io/about.html), and [Pedro L. C. Rodrigues](https://plcrodrigues.github.io)\n",
    "         \n",
    "with some code snippets from [Olivier Grisel](http://ogrisel.com/) (leaf encoder)\n",
    "\n",
    "It is the most creative aspect of Data Science!\n",
    "\n",
    "We will use here the Titanic dataset.\n",
    "\n",
    "![title](titanic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dtypes of the different columns. You will observe that it contains columns that\n",
    "are explicitly marked as `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows you to do things like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "make_column_selector(dtype_include='category')(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to get quickly the names of the columns to treat as categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data contains both quantitative and categorical variables. These categorical have some predictive power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, x='pclass', y='survived', hue='sex', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is how to feed these non-quantitative features to a supervised learning model?\n",
    "\n",
    "## 1) Categorical encoding\n",
    "\n",
    " - Nearly always need some treatment\n",
    " - High cardinality can create very sparse data\n",
    " - Difficult to impute missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) One-Hot encoding\n",
    "\n",
    "**Idea:** Each category is coded as a 0 or 1 in a dedicated column.\n",
    "\n",
    " - It is the most basic method. It is used with most linear algorithms\n",
    " - Drop first column to avoid collinearity\n",
    " - It uses sparse format which is memory-friendly\n",
    " - Most current implementations don’t gracefully treat missing, unseen variables\n",
    "\n",
    "Example with the `embarked` column. We have here 3 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a [scikit-learn OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(df1.head(10)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which column corresponds to what you can look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the first column will be a 1 if category was 'C', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(df1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now 4 columns, one corresponding to NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the columns are linearly dependant after one-hot encoding you can drop one column with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder(drop='first').fit_transform(df1.head(10)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This avoids colinearity, which for example leads to slower optimization solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Ordinal encoding\n",
    "\n",
    "**Idea:** Each category is coded with a different integer. The order being **arbitrary**.\n",
    "\n",
    " - Give every categorical variable a unique numerical ID\n",
    " - Useful for non-linear tree-based algorithms (forests, gradient-boosting)\n",
    " - Does not increase dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "oe.fit_transform(df1.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 'C' will be coded as 0, 'Q' as a 1 and 'S' as a 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Count encoding\n",
    "\n",
    "**Idea:** Replace categorical variables with their count in the train set\n",
    "\n",
    "- Useful for both linear and non-linear algorithms\n",
    "- Can be sensitive to outliers\n",
    "- May add log-transform, works well with counts\n",
    "- Replace unseen variables with `1`\n",
    "- May give collisions: same encoding, different variables\n",
    "\n",
    "You'll need to install the `category_encoders` package with:\n",
    "\n",
    "    pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.CountEncoder().fit_transform(df1.head(10)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'S' is replaced by 7 as it appears 7 times in the fitted data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Label / Ordinal count encoding\n",
    "\n",
    "**Idea:** Rank categorical variables by count and use this rank as encoding value. It is an ordinal encoding where the value is taking from the frequence of each category.\n",
    "\n",
    "- Useful for both linear and non-linear algorithms\n",
    "- Not sensitive to outliers\n",
    "- Won’t give same encoding to different variables\n",
    "- Best of both worlds\n",
    "\n",
    "As it is not available in any package we will implement this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "class CountOrdinalEncoder(OrdinalEncoder):\n",
    "    \"\"\"Encode categorical features as an integer array\n",
    "    usint count information.\n",
    "    \"\"\"\n",
    "    def __init__(self, categories='auto', dtype=np.float64):\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the OrdinalEncoder to X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to determine the categories of each feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.handle_unknown = 'use_encoded_value'\n",
    "        self.unknown_value = np.nan\n",
    "        super().fit(X)\n",
    "        X_list, _, _ = self._check_X(X)\n",
    "        # now we'll reorder by counts\n",
    "        for k, cat in enumerate(self.categories_):\n",
    "            counts = []\n",
    "            for c in cat:\n",
    "                counts.append(np.sum(X_list[k] == c))\n",
    "            order = np.argsort(counts)\n",
    "            self.categories_[k] = cat[order]\n",
    "        return self\n",
    "\n",
    "coe = CountOrdinalEncoder()\n",
    "coe.fit_transform(pd.DataFrame(df1.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'S' is replace by 2 as it's the most frequent, then 'C' is 1 and 'Q' is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoding is robust to collision which can happen with the CountEncoder when certain categories happen the same number of times. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coe.fit_transform(pd.DataFrame(['es', 'fr', 'fr', 'en', 'en', 'es']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.CountEncoder().fit_transform(pd.DataFrame(['es', 'fr', 'fr', 'en', 'en', 'es']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Hash encoding\n",
    "\n",
    "**Idea:** Does “OneHot-encoding” with arrays of a fixed length.\n",
    "\n",
    "- Avoids extremely sparse data\n",
    "- May introduce collisions\n",
    "- Can repeat with different hash functions and bag result for small bump in accuracy\n",
    "- Collisions usually degrade results, but may improve it.\n",
    "- Gracefully deals with new variables (eg: new user-agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.hashing.HashingEncoder(n_components=4).fit_transform(df1.head(10).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6) Target encoding\n",
    "\n",
    "Encode categorical variables by their ratio of target (binary classification or regression)\n",
    "\n",
    "Formula reads:\n",
    "\n",
    "$$\n",
    "    \\text{TE}(X) = \\alpha\\big(n(X)\\big) E[ y | x=X ] +  \\Big(1 - \\alpha\\big(n(X)\\big)\\Big) E[y]\n",
    "$$\n",
    "\n",
    "where $n(X)$ is the count of category $X$ and $\\alpha$ is a monotonically increasing function bounded between 0 and 1.[1].\n",
    "\n",
    "- Add smoothing to avoid setting variable encodings to 0.\n",
    "```\n",
    "[1] Micci-Barreca, 2001: A preprocessing scheme for\n",
    "high-cardinality categorical attributes in classification\n",
    "and prediction problems.\n",
    "```\n",
    "\n",
    "You will need the [dirty cat](https://pypi.org/project/dirty-cat/) package. You can install it with:\n",
    "\n",
    "    pip install dirty_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/skrub-data/skrub.git -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dirty_cat as dc  # install with: pip install dirty_cat\n",
    "\n",
    "X = np.array(['A', 'B', 'C', 'A', 'B', 'B'])[:, np.newaxis]\n",
    "y = np.array([1  , 1  , 1  , 0  , 0  , 1])\n",
    "\n",
    "dc.TargetEncoder(clf_type='binary-clf').fit_transform(X, y)\n",
    "# If \\alpha was 1 you would get: [0.5, 0.66, 1, 0.5, 0.66, 0.66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "te = TargetEncoder(target_type='binary', cv=2, smooth=1)\n",
    "te.fit(X, y)\n",
    "te.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7) NaN encoding\n",
    "\n",
    "It is quite frequent in real life that **the fact one variable is missing\n",
    "has some predictive power**. For example in the Titanic dataset the 'deck'\n",
    "parameter is very often missing and it is missing often for passengers who\n",
    "did not have a proper cabin and therefore who were most likely to die.\n",
    "\n",
    "To inform your supervised model you can explicit encode the missingness\n",
    "with a dedicated column.\n",
    "\n",
    "You can do this with a [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = np.array([0, 1., np.nan, 2., 0.])[:, None]\n",
    "SimpleImputer(strategy='median', add_indicator=True).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or [MissingIndicator](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "X = np.array([0, 1., np.nan, 2., 0.])[:, None]\n",
    "MissingIndicator().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8) Polynomial encoding\n",
    "\n",
    "**Idea:** Encode interactions between categorical variables\n",
    "\n",
    "- Linear algorithms without interactions can not solve the XOR problem\n",
    "- A polynomial kernel *can* solve XOR\n",
    "\n",
    "![title](xor.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [1, 1], [1, 0], [0, 0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "PolynomialFeatures(include_bias=False, interaction_only=True).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9) To go beyond\n",
    "\n",
    "You can also use some form of embedding eg using a Neural Network to create dense embeddings from categorical variables.\n",
    "\n",
    "- Map categorical variables in a function approximation problem into Euclidean spaces\n",
    "- Faster model training.\n",
    "- Less memory overhead.\n",
    "- Can give better accuracy than 1-hot encoded.\n",
    "- See for example https://arxiv.org/abs/1604.06737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10) Playing with `skrub`\n",
    "\n",
    "The `TableVectorizer` class from `skrub` proposes **automatically** for you which kind of encoding seems the most appropriate for each feature in the dataframe. But beware, it is always important to understand what are the actual encodings (and what they actually mean) so to be sure that there are not bizarre choices from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "tv = TableVectorizer()\n",
    "df_tv = tv.fit_transform(df)\n",
    "for ti in tv.transformers_:\n",
    "    print(ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Binning\n",
    "\n",
    "See https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html\n",
    "\n",
    "[KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html) allows you to estimate non-linear model in the original feature space while only using a linear logistic regression. \n",
    "\n",
    "See this [example in regression](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html).\n",
    "\n",
    "What it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.randn(10, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KBinsDiscretizer(n_bins=2).fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['C' + str(i) for i in range(4)]\n",
    "A = np.array([[1, 0], [0, 0.5]])\n",
    "X = rng.randn(2048, 2) @ A + np.c_[np.zeros(2048), np.ones(2048)]\n",
    "Xbins = KBinsDiscretizer(n_bins=4, encode='ordinal').fit_transform(X)\n",
    "fig, ax = plt.subplots(figsize=(12,5), ncols=2, sharey=True)\n",
    "for i, axi in enumerate(ax):\n",
    "    bins, edges = np.histogram(X[:,i], bins=32)\n",
    "    edges = (edges[1:] + edges[:-1])/2\n",
    "    dx = edges[1] - edges[0]\n",
    "    pdf = bins / (np.sum(bins) * dx)\n",
    "    axi.plot(edges, pdf)\n",
    "    axi.set_xlim(-4, +4)\n",
    "    for j in range(4):\n",
    "        axi.scatter(X[Xbins[:,i] == j,i], np.ones(512), c=colors[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[Xbins[:,i] == j,i].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Scaling\n",
    "\n",
    "\n",
    "Scale to numerical variables into a certain range\n",
    "\n",
    "- Standard (Z) Scaling\n",
    "- MinMax Scaling\n",
    "- Root scaling\n",
    "- Log scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = 10 + rng.randn(10, 1)\n",
    "print(X)\n",
    "print(X.mean(), X.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsc = StandardScaler().fit_transform(X)\n",
    "print(Xsc)\n",
    "print(Xsc.mean(), Xsc.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "X = np.arange(1, 10)[:, np.newaxis]\n",
    "FunctionTransformer(func=np.log).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_earnings = pd.read_csv('./earnings.csv')\n",
    "df_earnings = df_earnings.dropna()\n",
    "y = df_earnings['earn']/1000\n",
    "y = y[y > 0]\n",
    "ylog = np.log(y)\n",
    "fig, ax = plt.subplots(figsize=(10,4), ncols=2)\n",
    "for axi, yplot in zip(ax, [y, ylog]):\n",
    "    sns.kdeplot(yplot, ax=axi, bw_adjust=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Leaf coding\n",
    "\n",
    "The following is an implementation of a trick found in:\n",
    "\n",
    "Practical Lessons from Predicting Clicks on Ads at Facebook\n",
    "Junfeng Pan, He Xinran, Ou Jin, Tianbing XU, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, Joaquin Quiñonero Candela\n",
    "International Workshop on Data Mining for Online Advertising (ADKDD)\n",
    "\n",
    "https://research.fb.com/wp-content/uploads/2016/11/practical-lessons-from-predicting-clicks-on-ads-at-facebook.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "class TreeTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"One-hot encode samples with an ensemble of trees\n",
    "    \n",
    "    This transformer first fits an ensemble of trees (e.g. gradient\n",
    "    boosted trees or a random forest) on the training set.\n",
    "\n",
    "    Then each leaf of each tree in the ensembles is assigned a fixed\n",
    "    arbitrary feature index in a new feature space. If you have 100\n",
    "    trees in the ensemble and 2**3 leafs per tree, the new feature\n",
    "    space has 100 * 2**3 == 800 dimensions.\n",
    "    \n",
    "    Each sample of the training set go through the decisions of each tree\n",
    "    of the ensemble and ends up in one leaf per tree. The sample if encoded\n",
    "    by setting features with those leafs to 1 and letting the other feature\n",
    "    values to 0.\n",
    "    \n",
    "    The resulting transformer learn a supervised, sparse, high-dimensional\n",
    "    categorical embedding of the data.\n",
    "    \n",
    "    This transformer is typically meant to be pipelined with a linear model\n",
    "    such as logistic regression, linear support vector machines or\n",
    "    elastic net regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.fit_transform(X, y)\n",
    "        return self\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.estimator_ = clone(self.estimator)\n",
    "        self.estimator_.fit(X, y)\n",
    "        self.binarizers_ = []\n",
    "        sparse_applications = []\n",
    "        estimators = np.asarray(self.estimator_.estimators_).ravel()\n",
    "        for t in estimators:\n",
    "            lb = LabelBinarizer(sparse_output=True)\n",
    "            X_leafs = t.tree_.apply(X.astype(np.float32))\n",
    "            sparse_applications.append(lb.fit_transform(X_leafs))\n",
    "            self.binarizers_.append(lb)\n",
    "        return hstack(sparse_applications)\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        sparse_applications = []\n",
    "        estimators = np.asarray(self.estimator_.estimators_).ravel()\n",
    "        for t, lb in zip(estimators, self.binarizers_):\n",
    "            X_leafs = t.tree_.apply(X.astype(np.float32))\n",
    "            sparse_applications.append(lb.transform(X_leafs))\n",
    "        return hstack(sparse_applications)\n",
    "\n",
    "\n",
    "boosted_trees = GradientBoostingClassifier(\n",
    "    max_leaf_nodes=5, learning_rate=0.1,\n",
    "    n_estimators=10, random_state=0,\n",
    ")\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "TreeTransform(boosted_trees).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Limiting yourself to LogisticRegression propose features to predict survival.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "y = df.survived.values\n",
    "X = df.drop(['survived', 'alive'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs')\n",
    "ct = make_column_transformer(\n",
    "    (make_pipeline(SimpleImputer(), StandardScaler()), ['age', 'pclass', 'fare']),\n",
    ")\n",
    "clf = make_pipeline(ct, lr)\n",
    "np.mean(cross_val_score(clf, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we consider a one hot encoding for the sex of the passenger too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "ct = make_column_transformer(\n",
    "    (make_pipeline(SimpleImputer(), StandardScaler()), ['age', 'pclass', 'fare']),\n",
    "    (OneHotEncoder(),['sex'])\n",
    ")\n",
    "clf = make_pipeline(ct, lr)\n",
    "np.mean(cross_val_score(clf, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do better !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# load Titanic dataset\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# use a simple imputer\n",
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "# instantiate the TableVectorizer with imputer\n",
    "tv = TableVectorizer(numerical_transformer=si)\n",
    "\n",
    "# choose a scaler so logistic regression is happy\n",
    "sc = StandardScaler()\n",
    "\n",
    "# choose logreg as classifier\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# make pipeline\n",
    "clf = make_pipeline(tv, sc, lr)\n",
    "\n",
    "# print cross val score for classification\n",
    "np.mean(cross_val_score(clf, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtv = tv.fit_transform(X, y)\n",
    "print(X.shape)\n",
    "print(Xtv.shape)\n",
    "tv.transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
