{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap on Decision Trees\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees encode a series of \"if\" and \"else\" choices, similar to how a person might makes a decision.\n",
    "However, which questions to ask, and how to proceed for each answer is entirely learned from the data.\n",
    "\n",
    "For example, if you wanted to create a guide to identifying an animal found in nature, you\n",
    "might ask the following series of questions:\n",
    "\n",
    "- Is the animal **bigger or smaller than a meter long**?\n",
    "    + *bigger*: does the animal **have horns**?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: is the animal wearing a collar\n",
    "    + *smaller*: does the animal **have two or four legs**?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "and so on.  This binary splitting of questions is the essence of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main **benefit of tree-based models** is that they require **little preprocessing of the data**.\n",
    "They can work with **variables of different types** (continuous and categorical) and are **invariant to scaling of the features**.\n",
    "\n",
    "Another benefit is that tree-based models are what is called **\"nonparametric\"**, which means they don't have a fix set of parameters to learn. Instead, **a tree model can become more and more flexible**, if given more data.\n",
    "In other words, the **number of free parameters grows with the number of samples** and is not fixed, as for example in linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(\n",
    "    centers=[[0, 0], [1, 1]], random_state=61526, n_samples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y)\n",
    "print(f\"The class labels are: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "for klazz, color in zip(classes, [\"tab:orange\", \"tab:blue\"]):\n",
    "    mask_sample_klazz = y == klazz\n",
    "    ax.scatter(\n",
    "        X[mask_sample_klazz, 0], X[mask_sample_klazz, 1],\n",
    "        color=color, label=klazz,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "plt.axis(\"square\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Feature #0\")\n",
    "_ = plt.ylabel(\"Feature #1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function to create this scatter plot by passing 2 variables: `data` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, labels, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    classes = np.unique(labels)\n",
    "    for klazz, color in zip(classes, [\"tab:orange\", \"tab:blue\"]):\n",
    "        mask_sample_klazz = labels == klazz\n",
    "        ax.scatter(\n",
    "            data[mask_sample_klazz, 0], data[mask_sample_klazz, 1],\n",
    "            color=color, label=klazz,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "    sns.despine()\n",
    "    ax.axis(\"square\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Feature #0\")\n",
    "    _ = plt.ylabel(\"Feature #1\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn a set of binary rule using a portion of the data. Using the rules learned, we will predict on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the decision boundaries found using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf, X, alpha=0.5\n",
    ")\n",
    "_ = plot_data(X_train, y_train, ax=display.ax_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we get the following classification on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li>Modify the depth of the tree and see how the partitioning evolves. </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interactive plot below, the regions are assigned blue and orange colors to indicate the predicted class for that region. The shade of the color indicates the predicted probability for that class (darker = higher probability), while white regions indicate an equal predicted probability for either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import plot_tree_interactive\n",
    "plot_tree_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rule for splitting in decision tree for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will go slightly more into details regarding how a tree is selecting the best partition. First, instead of using synthetic data, we will use a real dataset this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/penguins.csv\")\n",
    "dataset = dataset.dropna(subset=[\"Body Mass (g)\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a decision tree to classify the penguin species using their body mass as a feature. To simplify the problem will focus only the Adelie and Gentoo species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the column of interest\n",
    "dataset = dataset[[\"Body Mass (g)\", \"Species\"]]\n",
    "# Make the species name more readable\n",
    "dataset[\"Species\"] = dataset[\"Species\"].apply(lambda x: x.split()[0])\n",
    "# Only select the Adelie and Gentoo penguins\n",
    "dataset = dataset.set_index(\"Species\").loc[[\"Adelie\", \"Gentoo\"], :]\n",
    "# Sort all penguins by their body mass\n",
    "dataset = dataset.sort_values(by=\"Body Mass (g)\")\n",
    "# Convert the dataframe (2D) to a series (1D)\n",
    "dataset = dataset.squeeze()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look at the body mass distribution for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "dataset.groupby(\"Species\").plot.hist(ax=ax, alpha=0.7, legend=True, density=True)\n",
    "ax.set_ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead to look at the distribution, we can look at all samples directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset),\n",
    "                   hue=dataset.index)\n",
    "_ = ax.set_xlabel(dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build a tree, we want to find splits, one at the time, such that we partition the data in way that classes as \"unmixed\" as possible. Let's make a first completely random split to highlight the principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random state such we all have the same results\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(dataset.size)\n",
    "\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset),\n",
    "                   hue=dataset.index)\n",
    "ax.set_xlabel(dataset.name)\n",
    "ax.set_title(f\"Body mass threshold: {dataset[random_idx]} grams\")\n",
    "_ = ax.vlines(dataset[random_idx], -1, 1, color=\"red\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the split done, we seek for having two partitions for which the samples are as much as possible from a single class and contains as many samples as possible. In decision tree, we used a **criterion** to assess the quality of a split. The **entropy** is one of the statistic which can describe the class mixity in a partition. Let's compute the entropy for the full dataset, the set on the left of the threshold and the set on the right of the split.\n",
    "\n",
    "Let's first look at the entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(proportion=(0., 1.0))\n",
    "def plot_entropy(proportion=0.5):\n",
    "    ps = np.linspace(0, 1, 100)\n",
    "    ents = [entropy([p, 1-p]) for p in ps]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(ps, ents)\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.xlabel('Proportion of class 1')\n",
    "    plt.axvline(proportion, linestyle='--', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the entropy is 0 when you have no mixing between classes and it's maximal when you have 50% of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy = entropy(\n",
    "    dataset.index.value_counts(normalize=True)\n",
    ")\n",
    "parent_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_entropy = entropy(\n",
    "    dataset[:random_idx].index.value_counts(normalize=True)\n",
    ")\n",
    "left_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_entropy = entropy(\n",
    "    dataset[random_idx:].index.value_counts(normalize=True)\n",
    ")\n",
    "right_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the quality of the split by combining the entropies. This is known as the **information gain**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy - (left_entropy + right_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we should normalize the entropies with the number of samples in each sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_parent, labels_left, labels_right):\n",
    "    # compute the entropies\n",
    "    entropy_parent = entropy(labels_parent.value_counts(normalize=True))\n",
    "    entropy_left = entropy(labels_left.value_counts(normalize=True))\n",
    "    entropy_right = entropy(labels_right.value_counts(normalize=True))\n",
    "\n",
    "    n_samples_parent = labels_parent.size\n",
    "    n_samples_left = labels_left.size\n",
    "    n_samples_right = labels_right.size\n",
    "\n",
    "    # normalize with the number of samples\n",
    "    normalized_entropy_left = ((n_samples_left / n_samples_parent) * \n",
    "                               entropy_left)\n",
    "    normalized_entropy_right = ((n_samples_right / n_samples_parent) *\n",
    "                                entropy_right)\n",
    "\n",
    "    return (entropy_parent -\n",
    "            normalized_entropy_left - normalized_entropy_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(\n",
    "    dataset.index,\n",
    "    dataset[:random_idx].index,\n",
    "    dataset[random_idx:].index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can compute the information gain for all possible body mass thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain = pd.Series(\n",
    "    [information_gain(dataset.index, dataset[:idx].index, dataset[idx:].index)\n",
    "     for idx in range(dataset.size)],\n",
    "    index=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = all_information_gain.plot()\n",
    "_ = ax.set_ylabel(\"Information gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the maximum of the information gain corresponds to the split which best partitions our data. So we can check the corresponding body mass threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.vlines(\n",
    "    all_information_gain.idxmax(), -1, 1,\n",
    "    color=\"red\", linestyle=\"--\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(42)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y_no_noise = np.sin(4 * x) + x\n",
    "y = y_no_noise + rnd.normal(size=len(x))\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "_ = plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=2)\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-3, 3, 1000).reshape((-1, 1))\n",
    "y_test = reg.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_test.ravel(), y_test, color='tab:blue', label=\"prediction\")\n",
    "plt.plot(X.ravel(), y, 'C7.', label=\"training data\")\n",
    "_ = plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree allows us to estimate the signal in a non-parametric way,\n",
    "but clearly has some issues.  In some regions, the model shows high bias and\n",
    "under-fits the data\n",
    "(seen in the long flat lines which don't follow the contours of the data),\n",
    "while in other regions the model shows high variance and over-fits the data\n",
    "(reflected in the narrow spikes which are influenced by noise in single points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Take the above example and repeat the training/testing by changing depth of the tree.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**HINT:** You can use `from ipywidgets import interact` to make an interactive plot.\n",
    "\n",
    "Solution is in `solutions/01-interactive_tree_regression.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the splitting criterion for a Decision Tree for regression?\n",
    "\n",
    "When considering a decision tree for regression and the Mean Squared Error (MSE)\n",
    "as the loss the \"impurity\" is simply the variance of the leaf:\n",
    "\n",
    "$$\\mathrm{impurity}(l) = \\mathrm{Var}(\\{y_i, \\forall x_i \\in l\\})$$\n",
    "\n",
    "where $l$ is the leaf considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>\n",
    "             Compute the information gain when using MSE as loss and variance as impurity criterion.\n",
    "         </li>\n",
    "         <li>\n",
    "             Estimate the best first split for the above 1D dataset.\n",
    "         </li>\n",
    "         <li>\n",
    "             What is the complexity of your algorithm?\n",
    "         </li>\n",
    "         <li>\n",
    "             What would you change if you the loss is now the Mean Absolute Error (MAE)?\n",
    "         </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in: `solutions/01-tree_regression_impurity.py`\n",
    "\n",
    "You will check that you get the same values as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DecisionTreeRegressor(max_depth=1)\n",
    "reg.fit(X, y)\n",
    "print(reg.tree_.impurity)\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "_ = plot_tree(reg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
