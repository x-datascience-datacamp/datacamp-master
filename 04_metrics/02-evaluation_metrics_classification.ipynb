{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of classification metrics\n",
    "\n",
    "Author: [Thomas Moreau](https://tommoral.github.io/) and [Alexandre Gramfort](http://alexandre.gramfort.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "In binary classification we have two classes $\\{0, 1\\}$, often called Positive and Negative and we try to predict the class $y_i$ for each sample $X_i$. A simple example would be to try to predict whether a dog is present or not in a given image. In this case, the Positive class is the class of all photos of dogs and the Negative class includes all the other photos. The goal of the classifier is to predict, for each photo, if it is Positive (P) or Negative (N): is there a dog in the photo?\n",
    "\n",
    "An interesting way to visualize the performances of the classifier is to look at the confusion matrix of the classifier.\n",
    "\n",
    "<img src=\"images/confusion_dog.png\" width=\"450\"/>\n",
    "\n",
    "This confusion matrix displays all the information about the performance of our classifier.\n",
    "We can see for each class whether or not it is well classified. In this example, 2 photos with dogs were classified as Negative (no dog!), and 1 photo without a dog was classified as Positive (dog!).\n",
    "\n",
    "When a Positive sample is falsely classified as Negative, we call this a False Negative (FN). And similarly, when a Negative sample is falsely classified as a Positive, it is called a False Positive.\n",
    "\n",
    "Classification metrics are commonly derived from the confusion matrix. We now present some of these metrics.\n",
    "\n",
    "See also https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification problem and train a classifier to solve it.\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data, target = make_classification(\n",
    "    n_samples=400, n_features=20, n_classes=2,\n",
    "    weights=[.7, .3], random_state=42, flip_y=.2,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, random_state=0\n",
    ")\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix for our classifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_df = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    columns=['N', 'P'],\n",
    "    index=['N', 'P']\n",
    ")\n",
    "sns.heatmap(cm_df, annot=True,\n",
    "            cmap='Oranges',)\n",
    "_ = plt.ylim(2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have recent scikit-learn you can also use the [`ConfusionMatrixDisplay`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy/ Zero-One Loss\n",
    "\n",
    "The idea of the accuracy is to measure how well your model predicted the class of the samples. This corresponds to the empirical risk of the model\n",
    "$$\n",
    "    Accuracy = \\frac{1}{N}\\sum_{i=1}^N 1\\{\\widehat y_i = y_i\\} \\enspace,\n",
    "    \\quad Error = 1 - Accuracy = \\frac{1}{N}\\sum_{i=1}^N 1\\{\\widehat y_i \\neq y_i\\} \\enspace.\n",
    "$$\n",
    "It corresponds to the probability of correct classification using the classifier.\n",
    "This is the most common metric used to evaluate a classification task.  \n",
    "Given a confusion matrix $C$, the accuracy is computed as: $ \\frac{\\text{Tr}(C)}{N} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", (y_pred == y_test).mean())\n",
    "print(\"Score scikit-learn: \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accuracy will not be 0.5 at chance level for any classification problem. It will only be the case if the dataset is balanced. The scikit-learn [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) is good way to evaluate what is chance level for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Score scikit-learn: \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take into account the class imbalance you can use the **balanced accuracy** as score.\n",
    "The balanced accuracy takes into account the proportions of each class.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score for the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "print(\"Score scikit-learn: \", balanced_accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "The precision and recall are both defined relatively to a chosen positive class.\n",
    "Precision is the fraction of positive instances among the instances predicted as positive (also called positive predictive value, PPV), while recall (also known as sensitivity) is the fraction of the total amount of positive instances that were actually retrieved.\n",
    "\n",
    "<img src=\"images/precision_recall.png\" width=\"750\"/>\n",
    "\n",
    "These two metrics are computed as\n",
    "$$\n",
    "        Precision = \\frac{TP}{TP+FP}\n",
    "        \\quad, \\quad\n",
    "        Recall = \\frac{TP}{TP+FN} \\enspace.\n",
    "$$\n",
    "\n",
    "While recall is also known as `sensitivity`, the `specificity` (a.k.a. true negative rate) is defined as:\n",
    "\n",
    "$$\n",
    "        Specificity = \\frac{TN}{TN+FP} \\enspace.\n",
    "$$\n",
    "\n",
    "Taking our example back, with positive class `1`, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "print(f\"Precision : {precision}\")\n",
    "print(f\"Recall : {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two metrics better describe the performance of the classifier, which tends to predict a majority of positive value. We can see in this example that the recall -- *i.e.* the recovery rate of the positive class -- is only at 60% while we have a precision of 80%. This means that our model is conservative. the few examples classified as positive are well chosen but to reach this good precision, we drop a lot of the other positive samples.\n",
    "\n",
    "It is usefull to note that these two metrics are complementary. They result from a tradeoff between the fact that the predictions are trustworthy (when the model predicts positive, it is confident that it will be true) and the fact that you don't reject too many samples (when a positive sample is presented to the model, it will be predicted as positive). To illustrate this concept, let's consider the limit case:\n",
    "\n",
    "* If a model always predicts samples as negative, it has a precision of 1 but it has a recall of 0 as it never finds the positive samples.\n",
    "* Inversely, if a model always predicts samples as positive, it has a recall of 1 (all the positive samples are classified as positive) but it as a poor precision as all negative samples are also classified as positive. \n",
    "\n",
    "\n",
    "### F1 score\n",
    "\n",
    "The inherent issue with precision and recall is that you can't optimize for both at the same time. As they result of a tradeoff, when you maximize one of the metric, the other will automatically decrease. A convenient way to balance these two metrics is to use the F1-score. This metric combines both the precision and recall and tries to balance both. This score is computed as the harmonic mean of the precision $P$ and recall $R$ *i.e.*\n",
    "\n",
    "$$\n",
    "    \\text{F1-score} = 2 * \\frac{P . R}{P + R}\\enspace.\n",
    "$$\n",
    "The use of a harmonic mean permits to give more importance to low scores. Thus, a precision and recall of $(0.8, 0.8)$ will have a larger F1-score compared to $(1, 0.6)$, unlike with arithmetic mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1-score : {f1_score(y_test, y_pred, pos_label=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is often used for model selection, to ensure a **good balance between the precision and recall** in the final model.\n",
    "\n",
    "The classic F1-score give the same weight to the precision and the recall. But it is also possible to **change balance between precision and recall** by using a modified f1-score for a given value $\\beta > 0$\n",
    "$$\n",
    "    \\text{F1-score}_\\beta =  (1 + \\beta^2) * \\frac{P . R}{\\beta^2 P + R} \\enspace .\n",
    "$$\n",
    "Setting $\\beta = 2$ gives twice as much importance to the recall as to the precision while $\\beta = 1/2$ will do the opposite. You can use the slider below to see how the F1-score varies with $\\beta$, as a function of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t = np.linspace(1e-16, 1, 101)\n",
    "P, R = np.meshgrid(t, t)\n",
    "\n",
    "def plot_f1(beta):\n",
    "    F1 = (1 + beta*beta) * (P*R) / (beta*beta*P+R)\n",
    "    plt.contourf(P, R, F1, 25, cmap='viridis')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.clim([0, 1])\n",
    "    plt.colorbar()\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "_ = interact(plot_f1, beta=widgets.FloatLogSlider(\n",
    "    value=1, min=-1, max=1, step=0.05,\n",
    "    description='$\\\\beta$')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC / AUC\n",
    "\n",
    "We take back our `LogisticRegression` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at the results from our classifier is the ROC curve (Receiver Operating Characteristic). Quoting Wikipedia :\n",
    "\n",
    "> A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.”\n",
    "\n",
    "In other words, if the classifier provides a decision function that can be thresholded to control false positives versus false negatives, the ROC curve summarises the different tradeoffs that can be achieved by varying this threshold.\n",
    "\n",
    "The computation of this curve requires the true labels $y_i$ and the target scores $p_i$, which can either be probability estimates of the positive class, confidence values, or binary decisions. The curve represents the True Positive Rate as a function of the False Positive Rate when varying the decision threshold. In other words, for a decision threshold $\\alpha \\in [0, 1]$, the ROC curve compute the FPR and TPR of a classifier such that $f(X_i) = 1\\{p_i > \\alpha)$. A perfect classifier, for which there exist a value $\\alpha$ such that all negative samples have a score lower than $\\alpha$ and all positive samples have a score higher, would have a ROC curve which would be a step function. With our example, the ROC curve gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:, 1], pos_label=1)\n",
    "plt.plot(fpr, tpr, label='ROC-curve')\n",
    "plt.plot([0, 0, 1], [0, 1, 1], label='perfect ROC-curve')\n",
    "plt.plot(range(0, 2), 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(ncol=2, bbox_to_anchor=(0, 1.05, 1., .3),\n",
    "           loc=3, borderaxespad=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this curve is convenient to visualize the performance of the classifier, it is usually summarized by the AUC metric which is the area under the ROC curve. This metric scales in $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, p_test, pos_label=1)\n",
    "plt.plot(fpr, tpr, label='ROC-curve')\n",
    "plt.plot(range(0, 2), 'k--')\n",
    "plt.fill_between(fpr, tpr, hatch='\\\\', alpha=0.3, label='AUC')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(ncol=2, bbox_to_anchor=(0, 1.05, 1., .3),\n",
    "           loc=3, borderaxespad=0)\n",
    "\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "print(f\"AUC : {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can do similar curves with precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, clf.predict_proba(X_test)[:, 1], pos_label=1\n",
    ")\n",
    "plt.plot(recall, precision, label='PR-curve')\n",
    "plt.plot([0, 1, 1], [1, 1, 0], label='perfect PR-curve')\n",
    "plt.plot([0, 1], [1, 0], 'k--')\n",
    "plt.fill_between(recall, precision, hatch='\\\\', alpha=0.3, label='AUC')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(ncol=2, bbox_to_anchor=(0, 1.05, 1., .3),\n",
    "           loc=3, borderaxespad=0)\n",
    "\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR-AUC : {pr_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class metrics\n",
    "\n",
    "Some metrics are essentially defined for binary classification tasks (e.g. F1-score, AUC). In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the average parameter.\n",
    "\n",
    " * **Macro-average**: simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    " * **Weighted-average**: accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n",
    " * **Micro-average**: gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
    "\n",
    "We will see in the second part of the class how imbalanced classes can be handled efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
