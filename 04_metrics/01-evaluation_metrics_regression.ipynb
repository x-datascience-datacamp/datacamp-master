{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of regression metrics\n",
    "\n",
    "Author: [Thomas Moreau](https://tommoral.github.io/) and [Alexandre Gramfort](http://alexandre.gramfort.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluating the model quality\n",
    "\n",
    "\n",
    "For supervised learning algorithms, where one wants to predict a target $y$ from a given input $X$, one needs to define a loss function. For a model $f_\\theta$, parametrized by $\\theta$, the goal of supervised learning can be written as\n",
    "$$\n",
    "    \\min_{\\theta \\in \\Theta} \\frac{1}{N}\\sum_{i=1}^N \\ell(y_i, f_\\theta(X_i))]\n",
    "$$\n",
    "for a well chosen loss function $\\ell$ which measures the discrepancy between the predicted $f_\\theta(X)$ and the true target $y$.\n",
    "\n",
    "To **evaluate the model**, one needs to use a set of unseen data -- called *test set* or *validation set* -- to estimate the generalization performances of the model. This generalization can be quantified by computing the same loss used for training. However, these loss are not always very informative as there value can be hard to interpret (scale, imbalance, $\\dots$).\n",
    "\n",
    "To quantify the performance of a regression model different evaluation metrics are possible. We review below classical metric choices.\n",
    "\n",
    "The simple way to use a scoring metric during cross-validation is via the scoring parameter of [`sklearn.model_selection.cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate).\n",
    "We will see now different metrics to evaluate the models.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The most classical way to measure distance is to use the the $\\ell_2$-norm. The MSE metric simply computes the average $\\ell_2$-distance between the prediction and the true target, *i.e.*\n",
    "$$\n",
    "    MSE(y, \\widehat y) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\widehat y_i)^2\n",
    "$$\n",
    "\n",
    "While this metric can be useful to evaluate linear regression models with gaussian noise, it can fail to distinguish between good and bad cases when the assumptions of the linear model are broken. This is illustrated with the [Anscombe Quartet](https://fr.wikipedia.org/wiki/Quartet_d%27Anscombe), where 4 very different datasets have the same MSE error for linear regression. Note that as the `LinearRegression` fo scikit-learn is an Ordinary Least Squares, the reported MSE is the lowest one that can be achieved for a linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "with open('anscombe_quartet.json') as f:\n",
    "    anscombe_quartet = np.array(json.load(f))\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for data, ax in zip(anscombe_quartet, axes.ravel()):\n",
    "    x, y = data[0], data[1]\n",
    "    reg.fit(x[:, None], y)\n",
    "    y_pred = reg.predict(x[:, None])\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    t = np.arange(*xlim)\n",
    "    ax.plot(t, reg.coef_ * t + reg.intercept_, 'k--')\n",
    "    ax.set_xlim(xlim)\n",
    "    \n",
    "    ax.set_title(f\"RMSE = {np.sqrt(mean_squared_error(y_pred, y)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric can also be rescaled to take into account the variance of the original $y$. This is called the Explained Variance:\n",
    "$$\n",
    "    Var_{explained}(y, \\widehat y) = \\frac{\\sum_{i=1}^N (y_i - \\widehat y_i)^2\n",
    "                                         }{\\sum_{i=1}^N (y_i - \\bar y)^2}\n",
    "                                   = \\frac{MSE(y, \\widehat y)}{Var(y)}\n",
    "    \\enspace ,\n",
    "$$\n",
    "where $\\bar y$ is the mean value of $y$. This value is also linked to the [Determination coefficient ($R^2$)](https://en.wikipedia.org/wiki/Coefficient_of_determination) as $R^2 = 1 - Var_{explained}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "# Shuffle the data\n",
    "data, target = shuffle(data, target, random_state=0)\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=10)\n",
    "\n",
    "print(\"R2 score:\", cross_val_score(regressor, data, target, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 score is convenient because it has a natural scaling: 1 is\n",
    "perfect prediction, and 0 is around chance\n",
    "\n",
    "Now let us see which houses are easier to predict. Feature 3 tells us whether the house is along Charles river or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 1],\n",
    "                      target[data[:, 3] == 1], cv=5))\n",
    "print(\"Not Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 0],\n",
    "                      target[data[:, 3] == 0], cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 1],\n",
    "                      target[data[:, 3] == 1], cv=5,\n",
    "                      scoring='neg_mean_squared_error'))\n",
    "print(\"Not Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 0],\n",
    "                      target[data[:, 3] == 0], cv=5,\n",
    "                      scoring='neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Another popular metric for regression is to use the $\\ell_1$-norm to quantify the difference between the predictions and the target *i.e.*\n",
    "$$\n",
    "    MAE(y, \\widehat y) = \\frac{1}{N}\\sum_{i=1}^N |y_i - \\widehat y_i| \\enspace .\n",
    "$$\n",
    "This metric is less sensitive to strong errors than the MSE, as it does not square the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-3, 3, 1000)\n",
    "plt.plot(t, abs(t))\n",
    "plt.plot(t, t*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this MAE metric on the [Anscombe Quartet](https://fr.wikipedia.org/wiki/Quartet_d%27Anscombe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "reg = LinearRegression()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for (x, y), ax in zip(anscombe_quartet, axes.ravel()):\n",
    "    reg.fit(x[:, None], y)\n",
    "    y_pred = reg.predict(x[:, None])\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    t = np.arange(*xlim)\n",
    "    ax.plot(t, reg.coef_ * t + reg.intercept_, 'k--')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(f\"MAE = {mean_absolute_error(y_pred, y):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the MAE metric is less sensitive to outliers.\n",
    "\n",
    "Now let's see if we can find a regressor model that aims to optimize something closer\n",
    "to the MAE. For this we will use the [HuberRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "reg = HuberRegressor()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for (x, y), ax in zip(anscombe_quartet, axes.ravel()):\n",
    "    reg.fit(x[:, None], y)\n",
    "    y_pred = reg.predict(x[:, None])\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    t = np.arange(*xlim)\n",
    "    ax.plot(t, reg.coef_ * t + reg.intercept_, 'k--')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(f\"MAE = {mean_absolute_error(y_pred, y):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit from this metric is that it permits to report an error in the correct unit, meaningful for an application. Taking back the boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = shuffle(data, target, random_state=0)\n",
    "\n",
    "print(cross_val_score(regressor, data, target, cv=5,\n",
    "                      scoring='neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale free metrics\n",
    "\n",
    "Depending on the application of the regression problem, it might be necessary to use a metric which quantifies the relative error instead of the absolute error. There exists various ways to define such a metric, in particular:\n",
    "\n",
    "* The Mean Absolute Pourcentage Error (MAPE): $\\displaystyle \\frac{1}{N}\\sum_{i=1}^N \\left| \\frac{y_i-\\widehat y_i}{y_i}\\right|~.$ This computes the error between the predicted value and the target, and rescales it compared to the value of the target $y$. This means that the same error of 1 won't have the same effect depending on the fact that $y=1$ or $y=1000$, which can be useful in various situation. It is [available in scikit-learn](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) starting from version 0.24.\n",
    "\n",
    "* The Mean Squared Log Error (MSLE): $\\displaystyle \\frac{1}{N}\\sum_{i=1}^N (\\log(1 + y_i) - \\log(1 + \\widehat y_i))^2$. This computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss. This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
