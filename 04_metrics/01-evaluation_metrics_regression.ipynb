{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of regression metrics\n",
    "\n",
    "Author: [Thomas Moreau](https://tommoral.github.io/) and [Alexandre Gramfort](http://alexandre.gramfort.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluating the model quality\n",
    "\n",
    "\n",
    "For supervised learning algorithms, where one wants to predict a target $y$ from a given input $X$, one needs to define a loss function. For a model $f_\\theta$, parametrized by $\\theta$, the goal of supervised learning can be written as\n",
    "$$\n",
    "    \\min_{\\theta \\in \\Theta} \\frac{1}{N}\\sum_{i=1}^N \\ell(y_i, f_\\theta(X_i))\n",
    "$$\n",
    "for a well chosen loss function $\\ell$ which measures the discrepancy between the predicted $f_\\theta(X)$ and the true target $y$.\n",
    "\n",
    "To **evaluate the model**, one needs to use a set of unseen data -- called *test set* or *validation set* -- to estimate the generalization performances of the model. This generalization can be quantified by computing the same loss used for training. However, these loss are not always very informative as there value can be hard to interpret (scale, imbalance, $\\dots$).\n",
    "\n",
    "To quantify the performance of a regression model different evaluation metrics are possible. We review below classical metric choices.\n",
    "\n",
    "The simple way to use a scoring metric during cross-validation is via the scoring parameter of [`sklearn.model_selection.cross_validate`.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate).\n",
    "We will see now different metrics to evaluate the models.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The most classical way to measure distance is to use the the $\\ell_2$-norm. The MSE metric simply computes the average $\\ell_2$-distance between the prediction and the true target, *i.e.*\n",
    "$$\n",
    "    MSE(y, \\widehat y) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\widehat y_i)^2\n",
    "$$\n",
    "\n",
    "While this metric can be useful to evaluate linear regression models with gaussian noise, it can fail to distinguish between good and bad cases when the assumptions of the linear model are broken. This is illustrated with the [Anscombe Quartet](https://fr.wikipedia.org/wiki/Quartet_d%27Anscombe), where 4 very different datasets have the same MSE error for linear regression. Note that as the `LinearRegression` fo scikit-learn is an Ordinary Least Squares, the reported MSE is the lowest one that can be achieved for a linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "with open('anscombe_quartet.json') as f:\n",
    "    anscombe_quartet = np.array(json.load(f))\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharey=True, sharex=True)\n",
    "\n",
    "for data, ax in zip(anscombe_quartet, axes.ravel()):\n",
    "    \n",
    "    x, y = data[0], data[1]\n",
    "    reg.fit(x[:, None], y)\n",
    "    y_pred = reg.predict(x[:, None])\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    t = np.arange(*xlim)\n",
    "    ax.plot(t, reg.coef_ * t + reg.intercept_, 'k--')\n",
    "    ax.set_xlim(xlim)\n",
    "    \n",
    "    ax.set_title(f\"RMSE = {np.sqrt(mean_squared_error(y_pred, y)):.2e}\")\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(np.array(data).T)\n",
    "    print(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i, data in enumerate(anscombe_quartet):\n",
    "    print(i)\n",
    "    df = pd.DataFrame(data.T)\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric can also be rescaled to take into account the variance of the original $y$. \n",
    "\n",
    "This is called the Explained Variance:\n",
    "$$\n",
    "    \\text{Var}_{\\text{explained}}(y, \\widehat y) = \\frac{\\sum_{i=1}^N (y_i - \\widehat y_i)^2\n",
    "                                         }{\\sum_{i=1}^N (y_i - \\bar y)^2}\n",
    "                                   = \\frac{\\text{MSE}(y, \\widehat y)}{\\text{Var}(y)}\n",
    "    \\enspace ,\n",
    "$$\n",
    "where $\\bar y$ is the mean value of $y$. \n",
    "\n",
    "This value is also linked to the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) as $R^2 = 1 - \\text{Var}_{\\text{explained}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# The Boston house-price data\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# Shuffle the data\n",
    "data, target = shuffle(data, target, random_state=0)\n",
    "regressor = RandomForestRegressor(n_estimators=10)\n",
    "\n",
    "print(\"R2 score:\", cross_val_score(regressor, data, target, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 score is convenient because it has a natural scaling: 1 is\n",
    "perfect prediction, and 0 is around chance\n",
    "\n",
    "Now let us see which houses are easier to predict. Feature 3 tells us whether the house is along Charles river or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 1],\n",
    "                      target[data[:, 3] == 1], cv=5))\n",
    "print(\"Not Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 0],\n",
    "                      target[data[:, 3] == 0], cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that despite the name, $R^2$ can be a negative value! See the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 1],\n",
    "                      target[data[:, 3] == 1], cv=5,\n",
    "                      scoring='neg_mean_squared_error'))\n",
    "print(\"Not Along Charles River:\",\n",
    "      cross_val_score(regressor, data[data[:, 3] == 0],\n",
    "                      target[data[:, 3] == 0], cv=5,\n",
    "                      scoring='neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [this](https://stackoverflow.com/a/72388329) StackOverflow answer:\n",
    "\n",
    "> When we use the term \"loss\" it is heavily implied that we want to **minimize** it. However when we talk about \"score\" it's implied that we want to **maximize** it (e.g. accuracy of a classifier). Therefore, when scoring a regression model, a maximal score would mean a minimal loss, i.e. maximize the negated loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Another popular metric for regression is to use the $\\ell_1$-norm to quantify the difference between the predictions and the target *i.e.*\n",
    "$$\n",
    "    MAE(y, \\widehat y) = \\frac{1}{N}\\sum_{i=1}^N |y_i - \\widehat y_i| \\enspace .\n",
    "$$\n",
    "This metric is less sensitive to strong errors than the MSE, as it does not square the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1.35\n",
    "t = np.linspace(-3, 3, 1000)\n",
    "plt.plot(t, abs(t))\n",
    "plt.plot(t, t*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this MAE metric on the [Anscombe Quartet](https://fr.wikipedia.org/wiki/Quartet_d%27Anscombe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for (x, y), ax in zip(anscombe_quartet, axes.ravel()):\n",
    "    reg.fit(x[:, None], y)\n",
    "    y_pred = reg.predict(x[:, None])\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    t = np.arange(*xlim)\n",
    "    ax.plot(t, reg.coef_ * t + reg.intercept_, 'k--')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(f\"MAE = {mean_absolute_error(y_pred, y):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the MAE metric is less sensitive to outliers.\n",
    "\n",
    "But we're in a rather **strange** situation: the model is trained to minimize one objective function but we evaluate its performance based on another metric!\n",
    "\n",
    "Let's see if we can find a regressor model that aims to optimize something closer\n",
    "to the MAE. \n",
    "\n",
    "For this we will use the [HuberRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "reg = HuberRegressor()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for (x, y), ax in zip(anscombe_quartet, axes.ravel()):\n",
    "    reg.fit(x[:, None], y)\n",
    "    y_pred = reg.predict(x[:, None])\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    t = np.arange(*xlim)\n",
    "    ax.plot(t, reg.coef_ * t + reg.intercept_, 'k--')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(f\"MAE = {mean_absolute_error(y_pred, y):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit from this metric is that it permits to report an error in the correct unit, meaningful for an application. \n",
    "\n",
    "Taking back the boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = shuffle(data, target, random_state=0)\n",
    "\n",
    "print(cross_val_score(regressor, data, target, cv=5,\n",
    "                      scoring='neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale free metrics\n",
    "\n",
    "Depending on the application of the regression problem, it might be necessary to use a metric which quantifies the relative error instead of the absolute error. There exists various ways to define such a metric, in particular:\n",
    "\n",
    "* The Mean Absolute Pourcentage Error (MAPE): $\\displaystyle \\frac{1}{N}\\sum_{i=1}^N \\left| \\frac{y_i-\\widehat y_i}{y_i}\\right|~.$ This computes the error between the predicted value and the target, and rescales it compared to the value of the target $y$. This means that the same error of 1 won't have the same effect depending on the fact that $y=1$ or $y=1000$, which can be useful in various situation. It is [available in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) starting from version 0.24.\n",
    "\n",
    "> [PLCR] Note that the MAPE will punish harder the over-predictions than the under-predictions. For instance, if the estimate is 10x the true value the MAPE is 9 whereas if it is 0.1x the MAPE is 0.9. In other words, the estimator will have a bias towards under-predictions, which might not be desirable. \n",
    "\n",
    "> **Question**: How would you train a linear regressor to minimize this score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "X, y = data, target\n",
    "est = LinearRegression()\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "print('usual training')\n",
    "mse_list = []\n",
    "mape_list = []\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    est.fit(X[train_idx], y[train_idx])\n",
    "    y_true = y[test_idx]\n",
    "    y_pred = est.predict(X[test_idx])\n",
    "    mse_list.append(mean_squared_error(y_true, y_pred))\n",
    "    mape_list.append(mean_absolute_percentage_error(y_true, y_pred))\n",
    "print('MSE:', np.mean(mse_list))\n",
    "print('MAPE:', np.mean(mape_list))\n",
    "\n",
    "print('')\n",
    "\n",
    "print('weighted training')\n",
    "mse_list = []\n",
    "mape_list = []\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    est.fit(X[train_idx], y[train_idx], sample_weight=1.0/np.abs(y[train_idx]))\n",
    "    y_true = y[test_idx]\n",
    "    y_pred = est.predict(X[test_idx])\n",
    "    mse_list.append(mean_squared_error(y_true, y_pred))\n",
    "    mape_list.append(mean_absolute_percentage_error(y_true, y_pred))\n",
    "print('MSE:', np.mean(mse_list))\n",
    "print('MAPE:', np.mean(mape_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we had a pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "print('weighted training')\n",
    "mse_list = []\n",
    "mape_list = []\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    pipeline.fit(X[train_idx], y[train_idx], sample_weight=1.0/np.abs(y[train_idx]))\n",
    "    y_true = y[test_idx]\n",
    "    y_pred = est.predict(X[test_idx])\n",
    "    mse_list.append(mean_squared_error(y_true, y_pred))\n",
    "    mape_list.append(mean_absolute_percentage_error(y_true, y_pred))\n",
    "print('MSE:', np.mean(mse_list))\n",
    "print('MAPE:', np.mean(mape_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has recently launched a new feature called **metadata routing** that facilitates the above procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(enable_metadata_routing=True)\n",
    "\n",
    "scaler = StandardScaler().set_fit_request(sample_weight=False)\n",
    "reg = LinearRegression().set_fit_request(sample_weight=True)\n",
    "\n",
    "pipeline = make_pipeline(scaler, reg)\n",
    "\n",
    "print('weighted training')\n",
    "mse_list = []\n",
    "mape_list = []\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    pipeline.fit(X[train_idx], y[train_idx], sample_weight=1.0/np.abs(y[train_idx]))\n",
    "    y_true = y[test_idx]\n",
    "    y_pred = est.predict(X[test_idx])\n",
    "    mse_list.append(mean_squared_error(y_true, y_pred))\n",
    "    mape_list.append(mean_absolute_percentage_error(y_true, y_pred))\n",
    "print('MSE:', np.mean(mse_list))\n",
    "print('MAPE:', np.mean(mape_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Mean Squared Log Error (MSLE): $\\displaystyle \\frac{1}{N}\\sum_{i=1}^N \\Big(\\log(1 + y_i) - \\log(1 + \\widehat y_i)\\Big)^2$. This computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss. This metric is best to use when targets have exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate more than an over-predicted estimate.\n",
    "\n",
    "> [PLCR] Consider a regression problem where we have to **predict the time taken** by an agent to deliver the food to customers (e.g. Deliveroo). Now, if the regression model which we built overestimates the delivery time, the delivery agent then gets a relaxation on the time he takes to deliver food and this small overestimation is acceptable. But the problem arises when the predicted delivery time is less than the actual trip takes, in this case, the delivery agent is more likely to miss the deadline, as a result, the customer reviews can be affected.\n",
    "\n",
    "> **Question**: How would you train a linear regressor to minimize this loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "stats.probplot(y, dist=\"norm\", plot=ax[0])\n",
    "stats.probplot(np.log(1+ y), dist=\"norm\", plot=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
