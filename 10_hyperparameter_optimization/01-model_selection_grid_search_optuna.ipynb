{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient hyperparameter search with GridSearchCV and optuna\n",
    "\n",
    "Authors:\n",
    "- [Alexandre Gramfort](https://alexandre.gramfort.net/)\n",
    "- [Thomas Moreau](https://tommoral.github.io/about.html)\n",
    "- [Pedro L. C. Rodrigues](https://plcrodrigues.github.io/)\n",
    "\n",
    "adapted from the work of Olivier Grisel and Andreas Mueller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models have hyperparameters that influence the complexity of the functions that they can learn. Think the depth of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/overfitting_underfitting_cartoon.svg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters, Over-fitting, and Under-fitting\n",
    "\n",
    "Unfortunately, there is no general rule on how to find the sweet spot, and so machine learning practitioners have to find the best trade-off of model-complexity and generalization by trying several hyperparameter settings. Hyperparameters are the internal knobs or tuning parameters of a machine learning algorithm (in contrast to model parameters that the algorithm learns from the training data -- for example, the weight coefficients of a linear regression model); the depth of a decision tree or the number of trees in a gradient boosting are such hyperparameters.\n",
    "\n",
    "Most commonly this \"hyperparameter tuning\" is done using a brute force search, for example over multiple values of ``max_depth``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv('datasets/regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('datasets/regression.test', header=None, sep='\\t')\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "y = df[0].values\n",
    "X = df.drop(0, axis=1).values\n",
    "\n",
    "cv = KFold(shuffle=True, n_splits=5, random_state=42)\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "\n",
    "# for each parameter setting do cross-validation:\n",
    "for max_depth in [1, 3, 5, 10, 20]:\n",
    "    reg.set_params(max_depth=max_depth)\n",
    "    scores = cross_val_score(reg, X, y, cv=cv, scoring=\"r2\")\n",
    "    print(f\"max_depth: {max_depth}, average score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function in scikit-learn, called ``validation_plot`` to reproduce the cartoon figure above. It plots one parameter, such as the number of neighbors, against training and validation error (using cross-validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth = np.arange(1, 20)\n",
    "    \n",
    "train_scores, test_scores = validation_curve(\n",
    "    reg, X, y, param_name=\"max_depth\",\n",
    "    param_range=max_depth, cv=cv,\n",
    "    scoring=\"r2\"\n",
    ")\n",
    "plt.plot(max_depth, train_scores.mean(axis=1), label=\"train R2\")\n",
    "plt.plot(max_depth, test_scores.mean(axis=1), label=\"test R2\")\n",
    "plt.ylabel('R2')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.xlim([1, max(max_depth)])\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to automatize hyperparameter search is to use a built-in class in scikit-learn: ``GridSearchCV``, which takes a dictionary describing the parameters to sweep and which model to train. This grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested.\n",
    "\n",
    "To inspect the training score on different folds, we can set parameter ``return_train_score`` to ``True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "max_depth = [1, 2, 3, 4, 5, 6, 7]\n",
    "param_grid = {'max_depth': max_depth}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid,\n",
    "                    cv=cv, verbose=3,\n",
    "                    return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about `GridSearchCV` is that it is a **meta-estimator**. It takes an estimator like `DecisionTreeRegressor` above, and creates a new estimator that behaves exactly the same - in this case, like a regressor.\n",
    "So we can call ``fit`` on it, to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What ``fit`` does is a bit more involved then what we did above. First, it runs the same loop with cross-validation, to find the best parameter combination.\n",
    "Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation), to build a single new model using the best parameter setting.\n",
    "\n",
    "> [PLCR] In other words, is does a sub-cross validation using smaller splits for X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as with all models, we can use ``predict`` or ``score``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the best parameters found by ``GridSearchCV`` in the ``best_params_`` attribute, and the best score in the ``best_score_`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the performance and much more for each set of parameter values by accessing the `cv_results_` attributes, which is a dictionary where each key is a string and each value is an array. It can therefore be used to make a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_tiny = cv_results[['param_max_depth', 'mean_test_score']]\n",
    "cv_results_tiny.sort_values(by='mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with using this score for evaluation, however. You might be making what is called a **multiple hypothesis testing error**: if you try several parameter settings, some of them will work better just by chance, and the score that you have obtained might not reflect how your model would perform on new unseen data.\n",
    "Therefore, it is good to split off a separate test-set before performing grid-search. This pattern can be seen as a training-validation-test split, and is common in machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/grid_search_cross_validation.svg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this very easily by splitting of some test data using ``train_test_split``, training ``GridSearchCV`` on the training set, and applying the ``score`` method to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "param_grid = {'max_depth': max_depth}\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid, cv=cv)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the parameters that were selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some practitioners go for an easier scheme, splitting the data simply into three parts, training, validation and testing. This is a possible alternative if your training set is very large, or it is infeasible to train many models using cross-validation because training a model takes very long.\n",
    "You can do this with scikit-learn for example by splitting of a test-set and then applying GridSearchCV with ShuffleSplit cross-validation with a single iteration:\n",
    "\n",
    "<img src=\"figures/train_validation_test2.svg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "param_grid = {'max_depth': max_depth}\n",
    "single_split_cv = ShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid, cv=single_split_cv, verbose=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster, but it might result in worse hyperparameters and therefore worse results as the error estimate on left-out data will have much more variance. In other words you're more likely to be unlucky!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Apply grid-search to find the best learning_rate and number of trees in a HistGradientBoostingRegressor.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in: `solutions/03-gbdt_grid_search_cv.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that we wanted to tune the hyperparameters of a regressor coming out from `skrub` such as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "est = make_pipeline(StandardScaler(), HistGradientBoostingRegressor())\n",
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the `learning_rate` from the regressor in this pipeline, we need help from `get_params` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "param_grid = {'histgradientboostingregressor__learning_rate': [0.1, 0.5, 1.0]}\n",
    "single_split_cv = ShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "grid = GridSearchCV(est, param_grid=param_grid, cv=single_split_cv, verbose=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did Grid Search actually do?**\n",
    "\n",
    "At this point, it is useful to pause and reflect:\n",
    "\n",
    "- How many models were trained in total?\n",
    "- How many of them performed clearly worse than the best one?\n",
    "- Did the grid resolution match the sensitivity of the model?\n",
    "\n",
    "Grid search does not *learn* from previous evaluations.\n",
    "Each configuration is tested independently, even if previous results already suggest it is unlikely to perform well.\n",
    "\n",
    "This observation motivates a more adaptive strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided hyper-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we did hyper-optimization by giving a list of values to be tried. \n",
    "\n",
    "We were able to automatically generated these values (randomly or not) by using `RandomSearchCV` or `GridSearchCV`.\n",
    "\n",
    "We can expect to do better by trying certain parameters which \"are more likely\" to optimize our problem given previous parameters.\n",
    "\n",
    "We will use `optuna` to do so.\n",
    "\n",
    "\n",
    "URL : https://pypi.org/project/optuna/\n",
    "\n",
    "See video : https://www.youtube.com/watch?v=J_aymk4YXhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float('x', -10, 10)\n",
    "    return (x-2)**2\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import samplers\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    df_train = pd.read_csv('datasets/regression.train', header=None, sep='\\t')\n",
    "    df_test = pd.read_csv('datasets/regression.test', header=None, sep='\\t')\n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "    y = df[0].values\n",
    "    X = df.drop(0, axis=1).values\n",
    "    \n",
    "    cat = trial.suggest_categorical(\"est\", [\"boost\", \"svr\"])\n",
    "    \n",
    "    if cat == 'boost':\n",
    "    \n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 10**-5, 10**0, log=True)\n",
    "        l2_regularization = trial.suggest_float('l2_regularization', 10**-5, 10**0, log=True)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 100)\n",
    "\n",
    "        reg = HistGradientBoostingRegressor(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            l2_regularization=l2_regularization, \n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    elif cat == 'svr':\n",
    "\n",
    "        C = trial.suggest_float('C', 1e-3, 1e1, log=True)\n",
    "        reg = SVR(C=C)\n",
    "        \n",
    "    return np.mean(cross_val_score(reg, X, y, cv=5, n_jobs=-1, scoring=\"r2\"))\n",
    "\n",
    "sampler = samplers.TPESampler(seed=10)\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "optuna.logging.disable_default_handler()  # limit verbosity\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Show best result\n",
    "print(study.best_trial.params)\n",
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [t.value for t in study.trials]\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [t.value for t in study.trials]\n",
    "values = [np.max(values[:k]) for k in range(1, len(values))]\n",
    "plt.plot(values)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = HistGradientBoostingRegressor(random_state=42)\n",
    "params = study.best_trial.params\n",
    "params.pop('est')\n",
    "reg.set_params(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more please have a look at the doc of optuna. In particular:\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
