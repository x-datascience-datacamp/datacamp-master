{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced scikit-learn\n",
    "\n",
    "Authors: [Alexandre Gramfort](http://alexandre.gramfort.net), [Thomas Moreau](https://tommoral.github.io/about.html), and [Pedro L. C. Rodrigues](https://plcrodrigues.github.io/).\n",
    "\n",
    "The aim of this notebook is:\n",
    "\n",
    "  - To explain the **full scikit-learn API** (estimators, transformers, classifiers, regressors, splitters)\n",
    "      - to explain how to assemble these objects in complex **pipelines with mixed data types** (numerical, categorical etc.) using `Pipeline` and `ColumnTransformer` objects.\n",
    "  - Have you **write your own transformer, splitter and classifier**.\n",
    "  \n",
    "## Table of contents\n",
    "\n",
    "* [1 Working only with numerical data](#workingnumerical)\n",
    "    * [1.1 Pandas preprocessing](#workingnumerical_pandas)\n",
    "    * [1.2 Making it less error prone using scikit-learn](#workingnumerical_errorprone)    \n",
    "* [2 Working only with categorical data](#workingcategorical)\n",
    "* [3 Combining both categorical and numerical data in the pipeline](#combining)\n",
    "* [4 From one split to cross-validation](#crossvalidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain these concepts we will start from a full working code based on the Titanic dataset. \n",
    "\n",
    "Then, we will deconstruct all the blocks and start writing our own Python classes.\n",
    "\n",
    "First, let's fetch the Titanic dataset directly from [OpenML](https://openml.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification task is to predict whether or not a person will survive the Titanic disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>QUESTIONS</b>:</p>\n",
    "    <ul>\n",
    "        <li>What would happen if you tried to fit a <tt>RandomForestClassifier</tt>?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Working only with numerical data <a class=\"anchor\" id=\"workingnumerical\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a model using only numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pandas preprocessing  <a class=\"anchor\" id=\"workingnumerical_pandas\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using scikit-learn, we will do some simple preprocessing using pandas. First, let's select only a few the numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['pclass', 'age', 'parch', 'fare']\n",
    "\n",
    "X_train_num = X_train[num_cols]\n",
    "X_test_num = X_test[num_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>QUESTIONS</b>:</p>\n",
    "    <ul>\n",
    "        <li>And now, what would happen if you tried to fit a <tt>RandomForestClassifier</tt>?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to look into a summary of the data that we try to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are some missing data, we can replace them with a mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_imputed = X_train_num.fillna(X_train_num.mean())\n",
    "X_train_num_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_num_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "    <li>What should we do if there are also missing values in the test set?</li>\n",
    "    <li>Process the test set so as to be able to compute the test score of the model.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/01-pandas_fillna_test.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Making it less error prone using scikit-learn <a class=\"anchor\" id=\"workingnumerical_errorprone\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides some \"transformers\" to preprocess the data. `sklearn.impute.SimpleImputer` is a transformer allowing for the same job than the processing done with Pandas. However, we will see later that it integrates greatly with other scikit-learn components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any estimator in scikit-learn, a transformer has a `fit` method which should be called on the training data to learn the required statistics. In the case of a mean imputer, we need to compute the mean for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute the values by the mean, we can use the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.transform(X_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, we should impute with the values computed in `fit` when imputing the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>What is a \"Transformer\"?</b>: <br/>\n",
    "\n",
    "A scikit-learn transform should implement at least these methods:\n",
    "\n",
    "<ul>\n",
    "    <li>fit(X, y=None)</li>\n",
    "    <li>transform(X)</li>\n",
    "    <li>get_params()</li>\n",
    "    <li>set_params(**kwargs)</li>  \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = imputer.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the attributes of our `imputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_attributes = [attr for attr in dir(imputer) if not attr.startswith('_')]\n",
    "public_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have among these attributes:\n",
    "\n",
    "- **parameters** (keys in get_params method output)\n",
    "- **methods** (fit, transform, etc.)\n",
    "- **estimated quantities** that appear after a `fit` (ending with `_`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_methods = [\n",
    "    attr for attr in dir(imputer)\n",
    "    if not attr.startswith('_') and\n",
    "    not attr.endswith('_') and\n",
    "    attr not in params]\n",
    "public_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.inverse_transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_attributes = [\n",
    "    attr for attr in dir(imputer)\n",
    "    if not attr.startswith('_') and\n",
    "    attr.endswith('_')]\n",
    "fit_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>What are the attributes of a RandomForestClassifier. You will decompose these in the 3 categories.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline\n",
    "\n",
    "We saw earlier that we should be careful when preprocessing data to avoid any \"data leak\" (i.e. reusing some knowledge from the training when testing our model). Scikit-learn provides the `Pipeline` class to make successive transformations. In addition, it will ensure that the right operations will be applied at the right time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                      RandomForestClassifier(n_estimators=200))\n",
    "model.fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative syntax using named \"steps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy='mean')),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=200))    \n",
    "])\n",
    "model.fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving your estimator in HTML for presentations, blog posts etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import estimator_html_repr\n",
    "\n",
    "with open('model.html', 'w') as fid:\n",
    "    fid.write(estimator_html_repr(model))\n",
    "\n",
    "# !open model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Pipeline steps\n",
    "\n",
    "A pipeline is a sequence of `steps`. Each `step` is a scikit-learn estimator. All steps except the last one are typically **transformers** (fit, fit_transform, transform methods) and the last step is a **classifier** or a **regressor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps  # accessing steps as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps  # accessing steps with their names as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[:1]  # slicing a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import is_classifier\n",
    "\n",
    "is_classifier(model[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose the pipeline and chain the operations manually (mimicking what the Pipeline object does internally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = model[:-1]\n",
    "classifier = model[-1]\n",
    "\n",
    "X_train_preproc = preprocessor.fit_transform(X_train_num, y_train)\n",
    "X_test_preproc = preprocessor.transform(X_test_num)\n",
    "\n",
    "classifier.fit(X_train_preproc, y_train)\n",
    "classifier.score(X_test_preproc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards a ColumnTransformer\n",
    "\n",
    "If we want to directly fit the model on `X_train`, we can select the numerical columns using  a `ColumnTransformer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ColumnTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_preprocessing = ColumnTransformer([\n",
    "    (\"imputer\", SimpleImputer(strategy='mean'), num_cols)\n",
    "])\n",
    "    \n",
    "model = Pipeline([\n",
    "    (\"numerical preproc.\", numerical_preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100)),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Working only with categorical data <a class=\"anchor\" id=\"workingcategorical\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns (even more string data types) are not supported natively by machine-learning algorithms and required some preprocessing step usually called encoding. The most classical categorical encoders are the `OrdinalEncoder` and the `OneHotEncoder`. Let's first see the `OrdinalEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['sex', 'embarked', 'pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train[cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    (\"ordinal_encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "categorical_preprocessing = ColumnTransformer([\n",
    "    (\"categorical_preproc\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"categorical_preproc\", categorical_preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing and updating parameters of a Pipeline\n",
    "\n",
    "Pipeline is yet another scikit-learn estimators. It therefore has the methods `set_params` and `get_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(classifier__n_estimators=666)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    There are many other types of ways to encode categorical variables.\n",
    "    <ul>\n",
    "        <li>Write your own categorical encoder CountEncoder. The idea is to replace categorical variables with their count in the train set.</li>\n",
    "        <li>Change the ordinal encoder in the pipeline above with an instance of your CountEncoder.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Your class will need to inherit from `BaseEstimator`, `TransformerMixin` in `sklearn.base` submodule.\n",
    "You will use the class `Counter` from the collections module in the standard library.\n",
    "\n",
    "You will your code on this toy example\n",
    "\n",
    "```python\n",
    ">>> X = np.array([\n",
    "...    [0, 2],\n",
    "...    [1, 3],\n",
    "...    [1, 1],\n",
    "...    [1, 1],\n",
    "... ])\n",
    ">>> ce = CountEncoder()\n",
    ">>> print(ce.fit_transform(X))\n",
    "[[1 1]\n",
    " [3 1]\n",
    " [3 2]\n",
    " [3 2]]\n",
    "```\n",
    "\n",
    "Solution is in `solutions/01-count_encoder.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Combining both categorical and numerical data in the pipeline <a class=\"anchor\" id=\"combining\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "    <li>Try to combine the numerical and categorical pipelines into a single <tt>ColumnTransformer</tt></li>\n",
    "        <li>Fit a <tt>RandomForestClassifier</tt> on the output of this feature engineering. How does the test score evolve?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/01b-full_column_transformer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 From one split to cross-validation <a class=\"anchor\" id=\"crossvalidation\"></a> [↑](#Table-of-contents)\n",
    "\n",
    "CV objects are parametrized to split data in multiple train/test splits.\n",
    "\n",
    "A splitter should implement a `split` method.\n",
    "\n",
    "Given a `model`, some data `X, y` and a splitter one can fit and score on\n",
    "all requested data splits. Functions to do this are `cross_val_score`\n",
    "(historical way) and `cross_validate` (more modern way).\n",
    "\n",
    "Let's first define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ordinal_encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "categorical_preprocessing = ColumnTransformer([\n",
    "    (\"categorical_preproc\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"categorical_preproc\", categorical_preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use a default 5-fold CV. You will see that there is a large amount of discrepancy among the test_score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X_df, y, cv=5)\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that default CV object (here 5-fold CV) is deterministic, while the distribution of \"survivor\" is not uniform in the dataset. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.astype(int).to_frame().groupby(y.index.values // 100).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this one needs stratified folds (so that the fraction of \"survivors\" is the same in each fold) but also to shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = cross_validate(model, X_df, y, cv=cv)\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance across folds is now much smaller which is great!\n",
    "\n",
    "Let's look at the cross-validation scheme with a pretty plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import plot_cv_indices\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "for shuffle, ax in zip([False, True], axes):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=shuffle)\n",
    "    plot_cv_indices(cv, X_df, y, ax=ax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html for more details and a list of the different CV objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own cross-validation object\n",
    "\n",
    "A splitter should implement the `split` and `get_n_splits` method. The `split` should return an iterable of tuples of indices and `get_n_splits` an integer corresponding to the number of splits/folds. If you know about `yield` and Python generators you can use these.\n",
    "\n",
    "Let's first see what we get with the `cv` object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in cv.split(X_df, y):\n",
    "    print(train_idx[:5], test_idx[:5])\n",
    "    print(f\"N. samples train: {len(train_idx)}  -- N. samples test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Imagine that the index of <code>y</code> gives you some provenance about the sample (e.g. which cohort of subjects in a clinical study). Write a splitter that allows to test the performance of a model on a left-out cohort. In other words, you will do as many splits as the number of unique values in <code>y.index.values</code>, and predict of each left-out cohort. To simulate this, we will modify the index variable <code>y</code>, just for educational purposes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/01c-splitter.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_with_provenance = y.copy()\n",
    "y_with_provenance.index = y_with_provenance.index.values // 200  # to easily mimic 5 cohorts\n",
    "n_splits = y_with_provenance.index.nunique()\n",
    "print(n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you're done with this notebook you can do the assignments on scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
