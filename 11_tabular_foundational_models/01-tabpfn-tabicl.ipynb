{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to **TabPFN** and **TabICL**\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/x-datascience-datacamp/datacamp-master/blob/main/11_tabular_foundational_models/01-tabpfn-tabicl.ipynb)\n",
    "\n",
    "Author: [Pedro L. C. Rodrigues](https://plcrodrigues.github.io) and [Thomas Moreau](https://tommoral.github.io)\n",
    "\n",
    "- **TabPFN** : Hollman et al. \"Accurate predictions on small data with a tabular foundation model\" (2025) [[link](https://www.nature.com/articles/s41586-024-08328-6)]\n",
    "- **TabICL** : Qu et al. \"TabICL: A Tabular Foundation Model for In-Context Learning on Large Data\" (2025) [[link](https://arxiv.org/abs/2502.05564)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python implementation of TabPFN is developed by people from [Prior Labs](https://docs.priorlabs.ai/overview) and follows the same API from `scikit-learn`. \n",
    "\n",
    "Note, however, that to use the last version of the TabPFN's foundational model, you will need to authenticate at HuggingFace, which can be a bit messy. Because of this, we will be focusing on TabPFN-V2, which should be more than enough. \n",
    "\n",
    "⚡ GPU Recommended: For optimal performance, use a GPU (even older ones with ~8GB VRAM work well; 16GB needed for some large datasets). On CPU, only small datasets (≲1000 samples) are feasible. Note that **this notebook can be run on Codalab with the top button**.\n",
    "\n",
    "First of all, you will need to install the package by as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tabpfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with TabPFN\n",
    "\n",
    "We investigate how TabPFN can be used for regression and compare his performance versus other classic regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "\n",
    "from tabpfn import TabPFNRegressor\n",
    "from tabpfn.constants import ModelVersion\n",
    "\n",
    "import pandas as pd, requests\n",
    "\n",
    "# Loading the Boston dataset\n",
    "cols = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
    "df_boston = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                        sep='\\\\s+', header=None, names=cols)\n",
    "\n",
    "print(df_boston.shape)\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pure numpy arrays\n",
    "X, y = df_boston.drop(columns=[\"MEDV\"]).values, df_boston[\"MEDV\"].values\n",
    "\n",
    "# Choose cross-validation strategy\n",
    "cv = KFold(shuffle=True, n_splits=5)\n",
    "\n",
    "# Instantiate TabPFN for regression\n",
    "# regressor_tabpfn = TabPFNRegressor()\n",
    "regressor_tabpfn = TabPFNRegressor.create_default_for_version(ModelVersion.V2)\n",
    "regressor_tabpfn.n_estimators = 1\n",
    "\n",
    "scores = []\n",
    "for idx_train, idx_test in tqdm(cv.split(X, y)):\n",
    "    X_train, y_train = X[idx_train], y[idx_train]\n",
    "    X_test, y_test = X[idx_test], y[idx_test]\n",
    "    regressor_tabpfn.fit(X_train, y_train)\n",
    "    scores.append(regressor_tabpfn.score(X_test, y_test))\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>What is happening in the <span><code>fit</code></span> method?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how a `RandomForestRegressor` and the `HistGradientRegressor` behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "regressor_rf = RandomForestRegressor()\n",
    "regressor_hgbr = HistGradientBoostingRegressor()\n",
    "est_dict = {'rf':regressor_rf, 'hgbr':regressor_hgbr}\n",
    "for key, value in est_dict.items():\n",
    "    scores = cross_val_score(value, X, y, cv=cv)\n",
    "    print(key, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TabPFN beats both baseslines by quite a margin. However, it took much more time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "df_california, targets = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "print(df_california.shape)\n",
    "df_california.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is bigger than the previous one, so let's see how TabPFN behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # let's avoid cross-val for time sake\n",
    "X, y = df_california.values, targets.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "regressor_tabpfn.fit(X_train, y_train)\n",
    "print(regressor_tabpfn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>Why do you think we exploded in memory?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible trick is to subsample the dataset and use an ensemble of TabPFN regressors as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_tabpfn.ignore_pretraining_limits = True\n",
    "regressor_tabpfn.n_estimators = 1\n",
    "regressor_tabpfn.inference_config = {\"SUBSAMPLE_SAMPLES\": 500}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "regressor_tabpfn.fit(X_train, y_train)\n",
    "print(regressor_tabpfn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even better, use a GPU :-) \n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/x-datascience-datacamp/datacamp-master/blob/main/11_tabular_foundational_models/01-tabpfn-tabicl.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_rf = RandomForestRegressor()\n",
    "regressor_hgbr = HistGradientBoostingRegressor()\n",
    "est_dict = {'rf':regressor_rf, 'hgbr':regressor_hgbr}\n",
    "for key, est in est_dict.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    est.fit(X_train, y_train)\n",
    "    score = est.score(X_test, y_test)\n",
    "    print(key, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the slides, we saw that **TabICL** can, in principle, scale to any number of samples, due to the way that rows and columns are embedded in its architecture. So should we try to use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tabicl # watch out for the python version!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the **TabICL** [documentation](https://github.com/soda-inria/tabicl) we notice that it currently does not work for regression... :'(\n",
    "\n",
    "At least for now... ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>Why can TabPFN do regression out-of-the-box whereas TabICL not?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with TabPFN and TabICL\n",
    "\n",
    "Let's switch to a classifcation problem first with a small then with a big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "df, target = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "X, y = df.values, target.values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in previous classes that we can not simply plug the Titanic dataset into standard scikit-learn estimators. First, it is necessary to pre-process the data, encode categorical features, etc. But what happens in TabPFN ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(shuffle=True, n_splits=5)\n",
    "\n",
    "# Instantiate TabPFN for classification\n",
    "from tabpfn import TabPFNClassifier\n",
    "clf_tabpfn = TabPFNClassifier.create_default_for_version(ModelVersion.V2)\n",
    "clf_tabpfn.n_estimators = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf_tabpfn.fit(X_train, y_train)\n",
    "print(clf_tabpfn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about TabICL ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabicl import TabICLClassifier\n",
    "clf_icl = TabICLClassifier(n_estimators=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf_icl.fit(X_train, y_train)\n",
    "print(clf_icl.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documention can help us : https://github.com/soda-inria/tabicl?tab=readme-ov-file#basic-integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "from tabicl import TabICLClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TableVectorizer(),  # automatically handles various data types\n",
    "    TabICLClassifier(n_estimators=1)  # beware of the default parameters!\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.33, random_state=42) # note that we pass the dataframe!\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>Why can TabPFN preprocess categorical features directly and TabICL needs a pipeline?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import tabular_pipeline\n",
    "est = tabular_pipeline('classifier')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.33, random_state=42) # note that we pass the dataframe!\n",
    "est.fit(X_train, y_train)\n",
    "print(est.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a larger dataset and see how **TabICL** behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "DATA_DIR = Path().parent / \"data\"\n",
    "\n",
    "url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
    "       \"/adult/adult.data\")\n",
    "\n",
    "local_filename =  DATA_DIR/ Path(url).name\n",
    "if not local_filename.exists():\n",
    "    print(\"Downloading Adult Census datasets from UCI\")\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    urlretrieve(url, local_filename)\n",
    "\n",
    "names = (\"age, workclass, fnlwgt, education, education-num, \"\n",
    "         \"marital-status, occupation, relationship, race, sex, \"\n",
    "         \"capital-gain, capital-loss, hours-per-week, \"\n",
    "         \"native-country, income\").split(', ') \n",
    "df = pd.read_csv(local_filename, names=names)\n",
    "df = df.rename(columns={'income': 'class'})\n",
    "\n",
    "columns_to_plot = [\n",
    "    \"age\",\n",
    "    \"education-num\",\n",
    "    \"capital-loss\",\n",
    "    \"capital-gain\",\n",
    "    \"hours-per-week\",\n",
    "    \"class\",\n",
    "]\n",
    "df = df[columns_to_plot]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "X, y = df.iloc[:,:-1].values, target.values\n",
    "y = (y == ' <=50K').astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_icl = TabICLClassifier(n_estimators=4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf_icl.fit(X_train, y_train)\n",
    "print(clf_icl.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf_hgbr = HistGradientBoostingClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf_hgbr.fit(X_train, y_train)\n",
    "print(clf_hgbr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
