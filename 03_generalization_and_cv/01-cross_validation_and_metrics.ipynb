{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your machine-learning model\n",
    "\n",
    "Authors: [Alexandre Gramfort](http://alexandre.gramfort.net), [Thomas Moreau](https://tommoral.github.io/about.html), and [Pedro L. C. Rodrigues](https://plcrodrigues.github.io/).\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [1 The benefits of cross-calidation](#benefitscv)\n",
    "    * [1.1 Load our dataset](#benefitscv_load)\n",
    "    * [1.2 Empirical error vs generalization error](#benefitscv_empirical)\n",
    "    * [1.3 A single error is not enough... what about the variance?](#benefitscv_single)\n",
    "    * [1.4 Effect of the sample size on the variance analysis](#benefitscv_effect)\n",
    "* [2 Comparing results versus baseline and chance level](#comparing)\n",
    "* [3 Choice of cross-validation](#choicecv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core question in machine learning is how to evaluate the performance of a model once it's parameters are estimated (i.e. the model has been trained). In this notebook, we aim at presenting how you should answer this question in a statistically sound way. First, we will present the benefits of using cross-validation for this task and then have a quick look at different strategies and metrics that one should use in supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 The benefits of cross-validation<a class=\"anchor\" id=\"benefitscv\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load our dataset<a class=\"anchor\" id=\"benefitscv_load\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate our discussion, we will use the [California housing](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html) dataset. \n",
    "\n",
    "This dataset is a regression problem in which we want to estimate the median housing value given housing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "y *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Empirical error vs. generalization error<a class=\"anchor\" id=\"benefitscv_empirical\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a regressor which will be a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first train our regressor on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our regressor is trained, we can check the regressor performance. For this purpose, we will use the mean absolute error which will give us an error in the native unit of the target, i.e. k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = regressor.predict(X)\n",
    "score = mean_absolute_error(y_pred, y)\n",
    "print(f\"In average, our regressor make an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION</b>:\n",
    "     <ul>\n",
    "     <li>Are you surprised to get such performance with this regressor?</li>\n",
    "     <li>Do you expect this regressor to perform this way in production?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly or not, our regressor is perfect. However, we are currently not able to confirm if our model would work on future unseen data. We can simulate this stage by splitting our dataset into two datasets and keep a dataset out of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_train)\n",
    "score = mean_absolute_error(y_pred, y_train)\n",
    "print(f\"In average, our regressor make an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating our regressor on the training data, we still get a perfect model. This error is called the **empirical error**. Let see, if we get as lucky on the left-out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "score = mean_absolute_error(y_pred, y_test)\n",
    "print(f\"In average, our regressor make an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and not. By evaluating our model on a left-out set, we are checking that our model will be able to work on unseen data and thus if it is able to generalize. This type of error is called the **generalization error** and is the one that any data scientist hopes to decrease when creating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION</b>:\n",
    "     <ul>\n",
    "     <li>Are we sure that our algorithm is robust?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Generalization error:</h3>\n",
    "     The aim of model training is to select the model $f$ out of a class of models $\\mathcal F$ that minimizes a measure of the risk. The risk is measured with a loss $l$ between the true value $y$ associated to $x$ and the prediction $f(x)$ and thus we want to find:  \n",
    "        $$\n",
    "        f^\\star = \\arg\\min_{f \\in \\mathcal F}\\mathbb E_{(x, y) \\sim \\pi}[l(f(x), y]\n",
    "        $$  \n",
    "    The issue is that we cannot compute the expectation $\\mathbb E_{(x, y) \\sim \\pi}$ because we don't know the input distribution $\\pi$. Therefore, we approximate it with a set of examples $\\{(x_1, y_1), \\dots (x_N, y_N)\\}$ drawn <i>i.i.d.</i> from $\\pi$ and use the <b>Empirical Risk Minimization</b> (ERM):\n",
    "        $$\n",
    "        \\widehat{f} = \\arg\\min_{f \\in \\mathcal F}\\frac1N\\sum_{i=1}^Nl(f(x_i), y_i]\n",
    "        $$\n",
    "    If the samples are drawn independently, we know that the error has a variance of $\\mathcal O\\left(\\frac{1}{\\sqrt{N}}\\right)$. Thus there is a gap between the minimizer of the risk and the minimizer of the empirical risk. If we optimize too much for the ERM, the gap might be big and the selected model will have bad performance on unseen data. This is what is called <b>overfitting</b>. To control, this, one need to have a measure of the risk independent from the measure of the risk which is used to select the model: the Empirical Risk on the test set!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 A single error is not enough... what about the variance?<a class=\"anchor\" id=\"benefitscv_single\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we were able to estimate the generalization error, we are indeed unable to know anything about the variance of our model and thus if it is robust or not. This is where the framework of cross-validation is used. Indeed, we can repeat our experiment and compute several time our generalization error and get intuition about the stability of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way that we can think of is to shuffle our data and split it into two sets as we previously did and repeat several time our experiment. In scikit-learn, using the function `cross_validate` with the cross-validation `ShuffleSplit` allows us to make such evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "\n",
    "result_cv = cross_validate(\n",
    "    regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2\n",
    ")\n",
    "result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our cross-validation is done, we see that we got a Python dictionary with information regarding the cross-validation. Let's use a pandas dataframe to easily explore the content of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cv = pd.DataFrame(result_cv)\n",
    "result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got information about the training and testing time which we can discard for the moment. However, we see that we got the `test_score` which is our generalization scores. We have 30 numbers because we repeated 30 times the same experiment by shuffling the data. Now, we can plot the distribution of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(-result_cv[\"test_score\"], kde=True, bins=20)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the generalization error is centered around 45.5 k\\\\$ and range from 44 k\\\\$ to 47 k\\\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li>Can we conclude anything about the performance of our regressor?</li>\n",
    "     <li>Can we conclude anything about our cross-validation analysis?</li>\n",
    "     </ul>\n",
    "    <b>Hint: </b>Plot the distribution of the target.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to know if we can use our cross-validation results, we should put them into perspective with the problem that we try to solve. To know more about this, we can check the distribution of the original target (which we should have done beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the median value range from 50 k\\\\$ up to 500 k\\\\$. Thus an error range of 3 k\\\\$ means that our cross-validation results can be trusted and do not suffer from an excessive variance. Regarding the performance of our model itself, we can see that making an error of 45 k\\\\$ would be problematic even more if this happen for housing with low value. However, we also see some limitation regarding the metric that we are using. Making an error of 45 k\\\\$ for a target at 50 k\\\\$ and at 500 k\\\\$ should not have the same impact. We should instead use the mean absolute percentage error which will give a relative error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li>Check the distribution of the training errors similarly to what we did for the generalization errors.</li>\n",
    "     <li>What can you conclude?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the empirical error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Effect of the sample size on the variance analysis<a class=\"anchor\" id=\"benefitscv_effect\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are quite lucky. Our dataset count many samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an experiment and reduce the number of samples and repeat the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cv_analysis(regressor, X, y):\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "    result_cv = pd.DataFrame(\n",
    "        cross_validate(\n",
    "            regressor, X, y, cv=cv,\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "    return y.size, (result_cv[\"test_score\"] * -1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li>Make an experiment by subsampling the dataset and plotting the distribution of the generalization errors.</li>\n",
    "     <li>What can you conclude?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_4.py\n",
    "sample_sizes = [100, 500, 1000, 5000, 10000, 15000, y.size]\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with a low number of samples, the variance is much larger. Indeed, for low number of sample, we cannot even trust our cross-validation and therefore cannot conclude anything about our regressor. Therefore, it is really important to make experiment with a large enough sample size to be sure about the conclusions which would be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Comparing results versus baseline and chance level<a class=\"anchor\" id=\"comparing\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we compare the generalization error by taking into account the target distribution. A good practice is to compare the generalization error with a dummy baseline and the chance level. In regression, we could use the `DummyRegressor` and predict the mean target without using the data. The chance level can be determined by permuting the labels and check the difference of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()\n",
    "result_dummy = cross_validate(\n",
    "    dummy, X, y, cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "score, permutation_score, pvalue = permutation_test_score(\n",
    "    regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1, n_permutations=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the generalization errors for each of the experiment. We see that even our regressor does not perform well, it is far above chances our a regressor that would predict the mean target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.concat(\n",
    "    [\n",
    "        result_cv[\"test_score\"] * -1,\n",
    "        pd.Series(result_dummy[\"test_score\"]) * -1,\n",
    "        pd.Series(permutation_score) * -1,\n",
    "    ], axis=1\n",
    ").rename(columns={\n",
    "    \"test_score\": \"Cross-validation score\",\n",
    "    0: \"Dummy score\",\n",
    "    1: \"Permuted score\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(final_result, kind=\"kde\")\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Choice of cross-validation<a class=\"anchor\" id=\"choicecv\"></a> [↑](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li>Is <code>ShuffleSplit</code> cross-validation always the best cross-validation to use?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example of some financial quotes. These are the value of company stocks with the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "symbols = {'TOT': 'Total Energy', 'XOM': 'Exxon', 'CVX': 'Chevron',\n",
    "           'COP': 'ConocoPhillips', 'VLO': 'Valero Energy'}\n",
    "\n",
    "quotes = pd.DataFrame()\n",
    "\n",
    "for symbol, name in symbols.items():\n",
    "    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'\n",
    "           'master/financial-data/{}.csv')\n",
    "    filename = \"data/{}.csv\".format(symbol)\n",
    "    if not os.path.exists(filename):\n",
    "        urlretrieve(url.format(symbol), filename)\n",
    "    this_quote = pd.read_csv(filename)\n",
    "    quotes[name] = this_quote['open']\n",
    "quotes.index = pd.to_datetime(this_quote['date'])\n",
    "\n",
    "quotes.drop([\"Total Energy\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "_ = quotes.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = quotes.drop(columns=[\"Chevron\"]), quotes[\"Chevron\"]\n",
    "regressor = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=30)\n",
    "result_cv = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        regressor, X, y, cv=cv,\n",
    "    )\n",
    ")\n",
    "print(f'Mean R2: {result_cv[\"test_score\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION</b>:\n",
    "     It seems that we have the perfect regressor. Does this seem normal to you?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the different types of cross-validation that are available in scikit-learn:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
